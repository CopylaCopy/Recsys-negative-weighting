{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:39.079734Z",
     "iopub.status.busy": "2023-03-27T23:51:39.079342Z",
     "iopub.status.idle": "2023-03-27T23:51:40.127368Z",
     "shell.execute_reply": "2023-03-27T23:51:40.126406Z",
     "shell.execute_reply.started": "2023-03-27T23:51:39.079698Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.stats import ttest_rel\n",
    "from typing import Union, Tuple\n",
    "\n",
    "\n",
    "def load_data(data_path, args):\n",
    "    raw_data = np.loadtxt(data_path, dtype=np.float, delimiter=args.delim, usecols=[0, 1, 2])\n",
    "    if args.implicit:\n",
    "        raw_data = raw_data[raw_data[:, 2] > 3]\n",
    "        raw_data[:, 2] = 1\n",
    "    users = list(set(raw_data[:, 0].astype(np.int)))\n",
    "    users.sort()\n",
    "    user_dict = {k: i for i, k in enumerate(users)}\n",
    "    items = list(set(raw_data[:, 1].astype(np.int)))\n",
    "    items.sort()\n",
    "    item_dict = {k: i for i, k in enumerate(items)}\n",
    "    for i in range(len(raw_data)):\n",
    "        raw_data[i, 0] = user_dict[raw_data[i, 0]]\n",
    "        raw_data[i, 1] = item_dict[raw_data[i, 1]]\n",
    "    return raw_data\n",
    "\n",
    "\n",
    "def build_user_item_matrix(ratings, n_user, n_item):\n",
    "    data = ratings[:, 2]\n",
    "    row_index = ratings[:, 0]\n",
    "    col_index = ratings[:, 1]\n",
    "    shape = (n_user, n_item)\n",
    "    return sparse.csr_matrix((data, (row_index, col_index)), shape=shape)\n",
    "\n",
    "\n",
    "def RMSE(estimation, truth):\n",
    "    truth_coo = truth.tocoo()\n",
    "    row_idx = truth_coo.row\n",
    "    col_idx = truth_coo.col\n",
    "    data = truth.data\n",
    "    pred = np.zeros(shape=data.shape)\n",
    "    for i in range(len(data)):\n",
    "        pred[i] = estimation[row_idx[i], col_idx[i]]\n",
    "    sse = np.sum(np.square(data - pred))\n",
    "    return np.sqrt(np.divide(sse, len(data)))\n",
    "\n",
    "\n",
    "def RMSE_with_ttest(estimation, old_estimation, truth):\n",
    "    truth_coo = truth.tocoo()\n",
    "    row_idx = truth_coo.row\n",
    "    col_idx = truth_coo.col\n",
    "    data = truth_coo.data\n",
    "    pred_dis = np.zeros(shape=data.shape)\n",
    "    old_pred_dis = np.zeros(shape=data.shape)\n",
    "    for i in range(len(data)):\n",
    "        pred_dis[i] = abs(estimation[row_idx[i], col_idx[i]] - data[i])\n",
    "        old_pred_dis[i] = abs(old_estimation[row_idx[i], col_idx[i]] - data[i])\n",
    "    _, p_value = ttest_rel(pred_dis, old_pred_dis)\n",
    "    sse = np.sum(np.square(pred_dis))\n",
    "    sse_old = np.sum(np.square(old_pred_dis))\n",
    "    return np.sqrt(np.divide(sse, len(data))), np.sqrt(np.divide(sse_old, len(data))), p_value\n",
    "\n",
    "\n",
    "def RMSE_weighted_with_t_test(estimation, old_estimation, val_confidence):\n",
    "    val_confidence_dense = val_confidence.toarray()\n",
    "    val_preference_dense = val_confidence_dense.copy()\n",
    "    val_preference_dense[val_preference_dense > 0] = 1\n",
    "    val_confidence_dense[val_confidence_dense == 0] = 1\n",
    "    old_error = val_confidence_dense * np.power(old_estimation - val_preference_dense, 2)\n",
    "    new_error = val_confidence_dense * np.power(estimation - val_preference_dense, 2)\n",
    "    _, p_val = ttest_rel(new_error.flatten(), old_error.flatten())\n",
    "    return np.sqrt(np.mean(new_error)), np.sqrt(np.mean(old_error)), p_val\n",
    "\n",
    "\n",
    "def roc_auc_grouped(labels: np.ndarray,\n",
    "                    predictions: np.ndarray,\n",
    "                    group_ids: np.ndarray,\n",
    "                    return_aucs_list: bool = False) -> Union[Tuple[float, float, int], np.ndarray]:\n",
    "    # efficient implementation of grouped auc, see test_metrics.py for the correctness check\n",
    "\n",
    "    # l_max = labels.max()\n",
    "    # l_min = labels.min()\n",
    "    # logging.info(str(l_max) + ' ' + str(l_min))\n",
    "    # labels = (labels > l_max * 0.8).astype(int)\n",
    "    # sort group_ids, predictions and labels jointly by (group_id, prediction) key\n",
    "    indices = np.lexsort((predictions, group_ids))\n",
    "    group_ids = group_ids[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # unique monotonic group_id\n",
    "    _, group_ids2 = np.unique(group_ids, return_inverse=True)\n",
    "    _, unique_counts = np.unique(group_ids, return_counts=True)\n",
    "\n",
    "    offsets = np.cumsum(unique_counts)\n",
    "    offsets = np.insert(offsets, 0, 0)\n",
    "\n",
    "    # number of negatives up to current element\n",
    "    nneg_thru = np.cumsum(1 - labels)\n",
    "\n",
    "    # number of negatives at the beginning of each group\n",
    "    group_starts = nneg_thru[offsets - 1]\n",
    "    group_starts[0] = 0\n",
    "\n",
    "    # number of negatives up to current element, restarting at each group\n",
    "    nneg = nneg_thru - group_starts[group_ids2]\n",
    "\n",
    "    # number of ordered pairs with the current element\n",
    "    inversions = (nneg * labels)\n",
    "\n",
    "    # number of negatives in each group\n",
    "    nneg_counts = nneg[offsets[1:] - 1]\n",
    "    npos_counts = unique_counts - nneg_counts\n",
    "\n",
    "    total_pairs = nneg_counts * npos_counts\n",
    "\n",
    "    # Number of ordered pairs in each group\n",
    "    ordered_pairs = np.bincount(group_ids2, weights=inversions)\n",
    "\n",
    "    aucs = ordered_pairs[total_pairs > 0] / total_pairs[total_pairs > 0]\n",
    "\n",
    "    if return_aucs_list:\n",
    "        return aucs\n",
    "    else:\n",
    "        return float(np.mean(aucs)), float(np.std(aucs)), int(np.sum(total_pairs > 0))\n",
    "\n",
    "\n",
    "def roc_auc_with_t_test(estimation, old_estimation, truth):\n",
    "    user_ids = np.repeat(np.array(range(estimation.shape[1])), estimation.shape[0])\n",
    "    aucs_old = roc_auc_grouped(truth.toarray().flatten(), old_estimation.flatten(), user_ids, True)\n",
    "    aucs_new = roc_auc_grouped(truth.toarray().flatten(), estimation.flatten(), user_ids, True)\n",
    "    _, p_value = ttest_rel(aucs_new, aucs_old)\n",
    "    return np.mean(aucs_new), np.mean(aucs_old), p_value\n",
    "\n",
    "\n",
    "def precision_at_k_grouped(labels: np.ndarray,\n",
    "                           predictions: np.ndarray,\n",
    "                           group_ids: np.ndarray,\n",
    "                           k: int = 10,\n",
    "                           return_precision_list: bool = False) -> Union[Tuple[float, float, int], np.ndarray]:\n",
    "    # efficient implementation of grouped precision@k, see test_metrics.py for the correctness check\n",
    "\n",
    "    # l_max = labels.max()\n",
    "    # l_min = labels.min()\n",
    "    # logging.info(str(l_max) + ' ' + str(l_min))\n",
    "    # labels = (labels > l_max * 0.8).astype(int)\n",
    "    # sort group_ids, predictions and labels jointly by (group_id, prediction) key\n",
    "    indices = np.lexsort((-predictions, group_ids))\n",
    "    group_ids = group_ids[indices]\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # 0000, 1111, 222, 3, 555555\n",
    "\n",
    "    # unique monotonic group_id\n",
    "    _, group_ids2 = np.unique(group_ids, return_inverse=True)\n",
    "    _, unique_counts = np.unique(group_ids, return_counts=True)\n",
    "\n",
    "    offsets = np.cumsum(unique_counts)\n",
    "    offsets = np.insert(offsets, 0, 0)\n",
    "\n",
    "    # independent indexing in each group. e.g., [0, 1, 2, 0, 0, 0, 1, 2, 3, 4, 0, 1, 2]\n",
    "    group_indices = np.arange(group_ids.shape[0]) - offsets[group_ids2]\n",
    "\n",
    "    # number of points in each group or k\n",
    "    denominator = np.minimum(unique_counts[group_ids2], np.repeat(k, group_ids.shape[0]))\n",
    "    pr_at_k_vals = labels / denominator\n",
    "\n",
    "    pr_at_k_vals[group_indices >= k] = 0\n",
    "\n",
    "    group_pr_at_k = np.zeros(unique_counts.shape[0])\n",
    "    np.add.at(group_pr_at_k, group_ids2, pr_at_k_vals)\n",
    "\n",
    "    if return_precision_list:\n",
    "        return group_pr_at_k\n",
    "    else:\n",
    "        return float(np.mean(group_pr_at_k)), float(np.std(group_pr_at_k)), group_pr_at_k.shape[0]\n",
    "\n",
    "\n",
    "def precision_at_10_with_t_test(estimation, old_estimation, truth):\n",
    "    user_ids = np.repeat(np.array(range(estimation.shape[1])), estimation.shape[0])\n",
    "    precisions_old = precision_at_k_grouped(truth.toarray().flatten(), old_estimation.flatten(), user_ids, 10, True)\n",
    "    precisions_new = precision_at_k_grouped(truth.toarray().flatten(), estimation.flatten(), user_ids, 10, True)\n",
    "    _, p_value = ttest_rel(precisions_new, precisions_old)\n",
    "    return np.mean(precisions_new), np.mean(precisions_old), p_value\n",
    "\n",
    "\n",
    "def u_emb_d_c(lamb, C, R, v, user_ind, vvt):\n",
    "    # calculates derivatives of each component of the embedding of user 'user_ind'\n",
    "    # wrt each confidence value of the user\n",
    "    # return shape (embedding_dim, num non-zero elements in C[user_ind]\n",
    "\n",
    "    # see test_gradients.py for the correctness check\n",
    "    idxs = np.argwhere(C[user_ind]).flatten()\n",
    "    m_inv = np.linalg.inv(lamb * np.eye(v.shape[1], v.shape[1]) + vvt + \\\n",
    "                          np.einsum('i,ik->ik', C[user_ind, idxs] - R[user_ind, idxs], v[idxs]).T.dot(v[idxs]))\n",
    "    outer_products = np.einsum('ij,il->ijl', v[idxs], v[idxs])\n",
    "    m_inv_v_outer = np.einsum('ij,kj->ki', m_inv, v[idxs])\n",
    "    m_inv_dot_outer_products = np.einsum('ij,cjk->cik', m_inv, outer_products)\n",
    "    first_part = np.einsum('cji,i->cj', m_inv_dot_outer_products,\n",
    "                           m_inv.dot(np.einsum('i,ik->k', C[user_ind, idxs], v[idxs])))\n",
    "    return -first_part + m_inv_v_outer\n",
    "\n",
    "\n",
    "def i_emb_d_c(lamb, C, R, u, item_ind, uut):\n",
    "    # calculates derivatives of each component of the embedding of item 'item_ind'\n",
    "    # wrt each confidence value of the item\n",
    "    # return shape (embedding_dim, num non-zero elements in C[:, item_ind]\n",
    "\n",
    "    # see test_gradients.py for the correctness check\n",
    "    idxs = np.argwhere(C[:, item_ind]).flatten()\n",
    "    m_inv = np.linalg.inv(lamb * np.eye(u.shape[1], u.shape[1]) + uut + \\\n",
    "                          np.einsum('i,ik->ik', C[idxs, item_ind] - R[idxs, item_ind], u[idxs]).T.dot(u[idxs]))\n",
    "    outer_products = np.einsum('ij,il->ijl', u[idxs], u[idxs])\n",
    "    m_inv_u_outer = np.einsum('ij,kj->ki', m_inv, u[idxs])\n",
    "    m_inv_dot_outer_products = np.einsum('ij,cjk->cik', m_inv, outer_products)\n",
    "    first_part = np.einsum('cji,i->cj', m_inv_dot_outer_products,\n",
    "                           m_inv.dot(np.einsum('i,ik->k', C[idxs, item_ind], u[idxs])))\n",
    "    return -first_part + m_inv_u_outer\n",
    "\n",
    "\n",
    "def loss_d_emb(confidence_val: np.ndarray,\n",
    "               preference_val: np.ndarray,\n",
    "               pred_val: np.ndarray,\n",
    "               user_embeddings: np.ndarray,\n",
    "               item_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # calculates derivatives of loss on validation\n",
    "    # wrt user embeddings and item embeddings\n",
    "    # (note that following the original paper we ignore regularization here)\n",
    "\n",
    "    # see test_gradients.py for the correctness check\n",
    "    error_weights = confidence_val.copy()\n",
    "    error_weights[error_weights == 0] = 1\n",
    "    print(error_weights.shape, (pred_val-preference_val).shape)\n",
    "    diffs = 2 * np.multiply(error_weights, (pred_val - preference_val))\n",
    "    print(diffs.shape, item_embeddings.shape)\n",
    "    grad_r_user = diffs.dot(item_embeddings)\n",
    "    grad_r_item = diffs.T.dot(user_embeddings)\n",
    "    return grad_r_user, grad_r_item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:40.130796Z",
     "iopub.status.busy": "2023-03-27T23:51:40.130318Z",
     "iopub.status.idle": "2023-03-27T23:51:40.416660Z",
     "shell.execute_reply": "2023-03-27T23:51:40.415776Z",
     "shell.execute_reply.started": "2023-03-27T23:51:40.130746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/implicit/gpu/__init__.py:14: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: CUDA driver version is insufficient for CUDA runtime version (/home/conda/feedstock_root/build_artifacts/implicit_1643471602441/work/./implicit/gpu/utils.h:71)'\n",
      "  f\"CUDA extension is built, but disabling GPU support because of '{e}'\",\n"
     ]
    }
   ],
   "source": [
    "#overload for fit method in als class for implementing negative weighting\n",
    "\n",
    "from implicit.utils import check_blas_config, check_random_state, nonzeros\n",
    "from implicit.cpu import _als\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from implicit.cpu.als import AlternatingLeastSquares as als\n",
    "\n",
    "log = logging.getLogger(\"implicit\")\n",
    "\n",
    "def check_csr(user_items):\n",
    "    if not isinstance(user_items, scipy.sparse.csr_matrix):\n",
    "        class_name = user_items.__class__.__name__\n",
    "        start = time.time()\n",
    "        user_items = user_items.tocsr()\n",
    "        warnings.warn(\n",
    "            f\"Method expects CSR input, and was passed {class_name} instead. \"\n",
    "            f\"Converting to CSR took {time.time() - start} seconds\",\n",
    "            ParameterWarning,\n",
    "        )\n",
    "    return user_items\n",
    "\n",
    "class als_with_weights(als):\n",
    "    def __init__(\n",
    "        self,\n",
    "        factors=100,\n",
    "        regularization=0.01,\n",
    "        alpha=1.0, \n",
    "        weights = False,\n",
    "        dtype=np.float32,\n",
    "        use_native=True,\n",
    "        use_cg=True,\n",
    "        iterations=15,\n",
    "        calculate_training_loss=False,\n",
    "        num_threads=0,\n",
    "        random_state=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            factors=factors,\n",
    "            regularization=regularization,\n",
    "            dtype=dtype,\n",
    "            use_native=use_native,\n",
    "            use_cg=use_cg,\n",
    "            iterations=iterations,\n",
    "            calculate_training_loss=calculate_training_loss,\n",
    "            num_threads=num_threads,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        self.alpha = alpha\n",
    "        self.weights = weights\n",
    "        \n",
    "        \n",
    "    def fit(self, user_items, show_progress=True, callback=None):\n",
    "        \"\"\"Factorizes the user_items matrix.\n",
    "        After calling this method, the members 'user_factors' and 'item_factors' will be\n",
    "        initialized with a latent factor model of the input data.\n",
    "        The user_items matrix does double duty here. It defines which items are liked by which\n",
    "        users (P_ui in the original paper), as well as how much confidence we have that the user\n",
    "        liked the item (C_ui).\n",
    "        The negative items are implicitly defined: This code assumes that positive items in the\n",
    "        user_items matrix means that the user liked the item. The negatives are left unset in this\n",
    "        sparse matrix: the library will assume that means Piu = 0 and Ciu = 1 for all these items.\n",
    "        Negative items can also be passed with a higher confidence value by passing a negative\n",
    "        value, indicating that the user disliked the item.\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_items: csr_matrix\n",
    "            Matrix of confidences for the liked items. This matrix should be a csr_matrix where\n",
    "            the rows of the matrix are the users, the columns are the items liked that user,\n",
    "            and the value is the confidence that the user liked the item.\n",
    "        show_progress : bool, optional\n",
    "            Whether to show a progress bar during fitting\n",
    "        callback: Callable, optional\n",
    "            Callable function on each epoch with such arguments as epoch, elapsed time and progress\n",
    "        \"\"\"\n",
    "        # initialize the random state\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        Cui = check_csr(user_items)\n",
    "        if Cui.dtype != np.float32:\n",
    "            Cui = Cui.astype(np.float32)\n",
    "\n",
    "        # Give the positive examples more weight if asked for\n",
    "#         if self.alpha != 1.0:\n",
    "#             Cui = self.alpha * Cui\n",
    "        if self.weights:   #negative weighting wiu = alpha*|Iu|\n",
    "            neg_Cui = csr_matrix(np.abs(Cui.astype(bool).toarray()-1))\n",
    "            sums = np.array(np.sum(Cui, axis = 1)).squeeze()      #weights can be really high, so just make them lower\n",
    "            neg_weights = diags(sums/np.mean(sums)/6)@neg_Cui\n",
    "            #tri = csr_matrix(np.random.random_integers(0, 1, (neg_weights.shape)))\n",
    "            val = np.prod(neg_weights.shape)\n",
    "            tri = np.zeros(val)\n",
    "            idx = np.random.choice(np.arange(0, val, 1), size = val//3, replace = False)\n",
    "            tri[idx]=1\n",
    "            shape = neg_weights.shape\n",
    "            neg_weights = csr_matrix(np.where(tri.reshape(shape)==1, neg_weights.toarray(), 0))           \n",
    "            Cui = neg_weights+Cui\n",
    "        s = time.time()\n",
    "        Ciu = Cui.T.tocsr()\n",
    "        log.debug(\"Calculated transpose in %.3fs\", time.time() - s)\n",
    "\n",
    "        items, users = Ciu.shape\n",
    "\n",
    "        s = time.time()\n",
    "        # Initialize the variables randomly if they haven't already been set\n",
    "        if self.user_factors is None:\n",
    "            self.user_factors = random_state.rand(users, self.factors).astype(self.dtype) * 0.01\n",
    "        if self.item_factors is None:\n",
    "            self.item_factors = random_state.rand(items, self.factors).astype(self.dtype) * 0.01\n",
    "\n",
    "        log.debug(\"Initialized factors in %s\", time.time() - s)\n",
    "\n",
    "        # invalidate cached norms and squared factors\n",
    "        self._item_norms = self._user_norms = None\n",
    "        self._YtY = None\n",
    "        self._XtX = None\n",
    "        loss = None\n",
    "\n",
    "        solver = self.solver\n",
    "\n",
    "        log.debug(\"Running %i ALS iterations\", self.iterations)\n",
    "        with tqdm(total=self.iterations, disable=not show_progress) as progress:\n",
    "            # alternate between learning the user_factors from the item_factors and vice-versa\n",
    "            for iteration in range(self.iterations):\n",
    "                s = time.time()\n",
    "                solver(\n",
    "                    Cui,\n",
    "                    self.user_factors,\n",
    "                    self.item_factors,\n",
    "                    self.regularization,\n",
    "                    num_threads=self.num_threads,\n",
    "                )\n",
    "                solver(\n",
    "                    Ciu,\n",
    "                    self.item_factors,\n",
    "                    self.user_factors,\n",
    "                    self.regularization,\n",
    "                    num_threads=self.num_threads,\n",
    "                )\n",
    "                progress.update(1)\n",
    "\n",
    "                if self.calculate_training_loss:\n",
    "                    loss = _als.calculate_loss(\n",
    "                        Cui,\n",
    "                        self.user_factors,\n",
    "                        self.item_factors,\n",
    "                        self.regularization,\n",
    "                        num_threads=self.num_threads,\n",
    "                    )\n",
    "                    progress.set_postfix({\"loss\": loss})\n",
    "\n",
    "                    if not show_progress:\n",
    "                        log.info(\"loss %.4f\", loss)\n",
    "\n",
    "                # Backward compatibility\n",
    "                if not callback:\n",
    "                    callback = self.fit_callback\n",
    "                if callback:\n",
    "                    callback(iteration, time.time() - s, loss)\n",
    "\n",
    "        if self.calculate_training_loss:\n",
    "            log.info(\"Final training loss %.4f\", loss)\n",
    "\n",
    "        self._check_fit_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:40.419116Z",
     "iopub.status.busy": "2023-03-27T23:51:40.418499Z",
     "iopub.status.idle": "2023-03-27T23:51:40.482037Z",
     "shell.execute_reply": "2023-03-27T23:51:40.480199Z",
     "shell.execute_reply.started": "2023-03-27T23:51:40.419079Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Process\n",
    "from numpy.linalg import inv\n",
    "from scipy import sparse\n",
    "import logging\n",
    "import implicit\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "def partition(ratings, seed, fold):\n",
    "    np.random.RandomState(seed).shuffle(ratings)\n",
    "    test_size = int(0.2 * ratings.shape[0])\n",
    "\n",
    "    test_data = ratings[:test_size]\n",
    "    val_data = dict()\n",
    "    fold_size = (ratings.shape[0] - test_size) // fold\n",
    "    for i in range(fold - 1):\n",
    "        val_data[i + 1] = ratings[test_size + i * fold_size: test_size + (i + 1) * fold_size]\n",
    "    val_data[fold] = ratings[test_size + (fold - 1) * fold_size:]\n",
    "    return val_data, test_data\n",
    "\n",
    "\n",
    "def data_split(data_path, args):\n",
    "    fold = args.fold\n",
    "    ratings = load_data(data_path, args)\n",
    "    n_users = int(max(ratings[:, 0]) + 1)\n",
    "    n_items = int(max(ratings[:, 1]) + 1)\n",
    "    max_rating = max(ratings[:, 2])\n",
    "    min_rating = min(ratings[:, 2])\n",
    "\n",
    "    val_data, test_data = partition(ratings, 0, fold)\n",
    "\n",
    "    lambda_dict = dict()\n",
    "    for i in range(fold):\n",
    "        if i == 0:\n",
    "            lambda_dict[1] = deepcopy(val_data[2])\n",
    "            for j in range(3, fold + 1):\n",
    "                lambda_dict[1] = np.vstack((lambda_dict[1], val_data[j]))\n",
    "        else:\n",
    "            lambda_dict[i + 1] = deepcopy(val_data[1])\n",
    "            for j in range(2, i + 1):\n",
    "                lambda_dict[i + 1] = np.vstack((lambda_dict[i + 1], val_data[j]))\n",
    "            for j in range(i + 2, fold + 1):\n",
    "                lambda_dict[i + 1] = np.vstack((lambda_dict[i + 1], val_data[j]))\n",
    "\n",
    "    zipped_index_dict = dict()\n",
    "    for i in range(fold):\n",
    "        zipped_index_dict[i + 1] = [(int(_[0]), int(_[1])) for _ in lambda_dict[i + 1]]\n",
    "\n",
    "    train_csr_dict = dict()\n",
    "    for i in range(fold):\n",
    "        train_csr_dict[i + 1] = build_user_item_matrix(lambda_dict[i + 1], n_users, n_items)\n",
    "    val_csr_dict = dict()\n",
    "    for i in range(fold):\n",
    "        val_csr_dict[i + 1] = build_user_item_matrix(val_data[i + 1], n_users, n_items)\n",
    "    test_csr = build_user_item_matrix(test_data, n_users, n_items)\n",
    "    return train_csr_dict, val_csr_dict, test_csr, zipped_index_dict, max_rating, min_rating\n",
    "\n",
    "\n",
    "def cf_ridge_regression(csr_matrix, reg_lambda, fixed_feature, update_feature):\n",
    "    n_feature = fixed_feature.shape[1]\n",
    "    for i in range(csr_matrix.shape[0]):\n",
    "        _, idx = csr_matrix[i, :].nonzero()\n",
    "        valid_feature = fixed_feature.take(idx, axis=0)\n",
    "        ratings = csr_matrix[i, idx].todense()\n",
    "        A_i = np.dot(valid_feature.T, valid_feature) + reg_lambda * np.eye(n_feature)\n",
    "        V_i = np.dot(valid_feature.T, ratings.T)\n",
    "        update_feature[i, :] = np.squeeze(np.dot(inv(A_i), V_i))\n",
    "\n",
    "\n",
    "def ALS(train_csr, args, n_iters, init_user_features=None, init_item_features=None):\n",
    "    if args.implicit:\n",
    "        logging.info('Implicit ALS, alpha {} max rating {}'.format(args.alpha, train_csr.data.max()))\n",
    "        model = als_with_weights(factors=args.factor, iterations=n_iters, weights = args.negative, #num_threads=args.als_threads,\n",
    "                                                     regularization=max(args.lambda_u, args.lambda_v),\n",
    "                                                     random_state=0)\n",
    "        model.fit(train_csr, show_progress=False)\n",
    "        return model.user_factors, model.item_factors\n",
    "    else:\n",
    "        user_features = 0.1 * np.random.RandomState(seed=0).rand(train_csr.shape[0], args.factor)\n",
    "        item_features = 0.1 * np.random.RandomState(seed=0).rand(train_csr.shape[1], args.factor)\n",
    "        if init_user_features is not None:\n",
    "            user_features = init_user_features\n",
    "        if init_item_features is not None:\n",
    "            item_features = init_item_features\n",
    "        train_csr_transpose = train_csr.T.tocsr()\n",
    "        for iteration in range(n_iters):\n",
    "            logging.info('Explicit ALS iteration {}'.format(iteration))\n",
    "            cf_ridge_regression(train_csr, args.lambda_u, item_features, user_features)\n",
    "            cf_ridge_regression(train_csr_transpose, args.lambda_v, user_features, item_features)\n",
    "        return user_features, item_features\n",
    "\n",
    "\n",
    "def grad_calc(train_csr, val_csr, zipped_index, user_features, item_features, args):\n",
    "    n_users = train_csr.shape[0]\n",
    "    n_items = train_csr.shape[1]\n",
    "\n",
    "    grad_r_user = np.zeros(shape=user_features.shape, dtype=np.float)\n",
    "    grad_r_item = np.zeros(shape=item_features.shape, dtype=np.float)\n",
    "\n",
    "    val_coo = val_csr.tocoo()\n",
    "    pred_val = np.dot(user_features, item_features.T)\n",
    "    for i, j, v in zip(val_coo.row, val_coo.col, val_coo.data):\n",
    "        loss = 2 * (pred_val[i, j] - v)\n",
    "        grad_r_user[i] += loss * item_features[j]\n",
    "        grad_r_item[j] += loss * user_features[i]\n",
    "\n",
    "    grad_user_m = np.zeros(shape=(train_csr.nnz, args.factor))\n",
    "    grad_user_dict = {}\n",
    "    cnt = 0\n",
    "    for i in range(n_users):\n",
    "        _, item_idx = train_csr[i, :].nonzero()\n",
    "        item_feat = item_features.take(item_idx, axis=0)\n",
    "        A = np.eye(args.factor, dtype=np.float) * args.lambda_u + np.dot(item_feat.T, item_feat)\n",
    "        grad_user_m_i = np.dot(item_features, inv(A))\n",
    "        for i_idx in item_idx:\n",
    "            tup = (i, i_idx)\n",
    "            grad_user_dict[tup] = cnt\n",
    "            grad_user_m[cnt] = grad_user_m_i[i_idx][:]\n",
    "            cnt += 1\n",
    "\n",
    "    train_csc = train_csr.tocsc()\n",
    "    grad_item_m = np.zeros(shape=(train_csc.nnz, args.factor))\n",
    "    grad_item_dict = {}\n",
    "    cnt = 0\n",
    "    for i in range(n_items):\n",
    "        user_idx, _ = train_csc[:, i].nonzero()\n",
    "        user_feat = user_features.take(user_idx, axis=0)\n",
    "        A = np.eye(args.factor, dtype=np.float) * args.lambda_v + np.dot(user_feat.T, user_feat)\n",
    "        grad_item_m_i = np.dot(user_features, inv(A))\n",
    "        for u_idx in user_idx:\n",
    "            tup = (i, u_idx)\n",
    "            grad_item_dict[tup] = cnt\n",
    "            grad_item_m[cnt] = grad_item_m_i[u_idx][:]\n",
    "            cnt += 1\n",
    "\n",
    "    row = [i for i, j in zipped_index]\n",
    "    col = [j for i, j in zipped_index]\n",
    "    data = [(np.dot(grad_r_user[i], grad_user_m[grad_user_dict[(i, j)]].T)\n",
    "             + np.dot(grad_r_item[j], grad_item_m[grad_item_dict[(j, i)]].T))\n",
    "            for i, j in zipped_index]\n",
    "    return sparse.coo_matrix((data, (row, col)), shape=(n_users, n_items)).tocsr()\n",
    "\n",
    "\n",
    "def grad_calc_implicit(train_csr, val_csr, zipped_index, user_features, item_features, args):\n",
    "    n_users = train_csr.shape[0]\n",
    "    n_items = train_csr.shape[1]\n",
    "\n",
    "    pred_val = np.dot(user_features, item_features.T)\n",
    "    confidence_val_dense = val_csr.toarray()\n",
    "    confidence_train_dense = train_csr.toarray()\n",
    "    preference_val_dense = confidence_val_dense.copy()\n",
    "    preference_val_dense[preference_val_dense > 0] = 1\n",
    "    preference_train_dense = confidence_train_dense.copy()\n",
    "    preference_train_dense[preference_train_dense > 0] = 1\n",
    "    grad_r_user, grad_r_item = loss_d_emb(confidence_val_dense,\n",
    "                                          preference_val_dense,\n",
    "                                          pred_val,\n",
    "                                          user_features,\n",
    "                                          item_features)\n",
    "\n",
    "    grad_user_m = np.zeros(shape=(train_csr.nnz, args.factor))\n",
    "    grad_user_dict = {}\n",
    "    cnt = 0\n",
    "    VVT = np.dot(item_features.T, item_features)\n",
    "    UUT = np.dot(user_features.T, user_features)\n",
    "\n",
    "    for i in range(n_users):\n",
    "        _, item_idx = train_csr[i, :].nonzero()\n",
    "        grad_user_m_i = u_emb_d_c(args.lambda_u, confidence_train_dense, preference_train_dense, item_features, i, VVT)\n",
    "        for item_num, i_idx in enumerate(item_idx):\n",
    "            tup = (i, i_idx)\n",
    "            grad_user_dict[tup] = cnt\n",
    "            grad_user_m[cnt] = grad_user_m_i[item_num][:]\n",
    "            cnt += 1\n",
    "\n",
    "    train_csc = train_csr.tocsc()\n",
    "    grad_item_m = np.zeros(shape=(train_csc.nnz, args.factor))\n",
    "    grad_item_dict = {}\n",
    "    cnt = 0\n",
    "    for i in range(n_items):\n",
    "        user_idx, _ = train_csc[:, i].nonzero()\n",
    "        grad_item_m_i = i_emb_d_c(args.lambda_v, confidence_train_dense, preference_train_dense, user_features, i, UUT)\n",
    "        for user_num, u_idx in enumerate(user_idx):\n",
    "            tup = (i, u_idx)\n",
    "            grad_item_dict[tup] = cnt\n",
    "            grad_item_m[cnt] = grad_item_m_i[user_num][:]\n",
    "            cnt += 1\n",
    "\n",
    "    row = [i for i, j in zipped_index]\n",
    "    col = [j for i, j in zipped_index]\n",
    "    data = [(np.dot(grad_r_user[i], grad_user_m[grad_user_dict[(i, j)]].T)\n",
    "             + np.dot(grad_r_item[j], grad_item_m[grad_item_dict[(j, i)]].T))\n",
    "            for i, j in zipped_index]\n",
    "    return sparse.coo_matrix((data, (row, col)), shape=(n_users, n_items)).tocsr()\n",
    "\n",
    "\n",
    "def grad_update_loop(train_csr, val_csr, zipped_index, max_rating, min_rating, args):\n",
    "    user_feature, item_feature = ALS(train_csr, args, args.als_iter)\n",
    "\n",
    "    A_i = train_csr\n",
    "    C_i = sparse.csr_matrix(train_csr.shape)\n",
    "    user_feature_i = user_feature\n",
    "    item_feature_i = item_feature\n",
    "    for i in range(args.debug_iter):\n",
    "        if args.implicit:\n",
    "            gradients = grad_calc_implicit(A_i, val_csr, zipped_index, user_feature_i, item_feature_i, args)\n",
    "        else:\n",
    "            gradients = grad_calc(A_i, val_csr, zipped_index, user_feature_i, item_feature_i, args)\n",
    "        A_i = A_i - gradients * args.debug_lr\n",
    "        for _ in range(A_i.nnz):\n",
    "            A_i.data[_] = min(max_rating, max(min_rating, A_i.data[_]))\n",
    "        logging.info(\"A_i mean {}, min {}, max {}\".format(A_i.data.mean(), A_i.data.min(), A_i.data.max()))\n",
    "        if args.retrain == \"full\":\n",
    "            user_feature_i, item_feature_i = ALS(A_i, args, args.als_iter)\n",
    "        if args.retrain == \"inc\":\n",
    "            user_feature_i, item_feature_i = ALS(A_i, args, 1, user_feature_i, item_feature_i)\n",
    "        C_i = A_i - train_csr\n",
    "    return C_i\n",
    "\n",
    "\n",
    "def get_path(args, part_id):\n",
    "    path = f\"./save/{args.dataset}/f{args.fold}_m{args.debug_iter}_lr{args.debug_lr}_part{part_id}_{args.retrain}\"\n",
    "    if args.implicit:\n",
    "        path += '_implicit'\n",
    "    return path + '.txt'\n",
    "\n",
    "\n",
    "def debug_process(train_csr, val_csr, zipped_index, max_rating, min_rating, id, args):\n",
    "    if args.implicit:\n",
    "        alpha = args.alpha\n",
    "    else:\n",
    "        alpha = 1\n",
    "    change_csr = grad_update_loop(alpha * train_csr, alpha * val_csr, zipped_index, max_rating, min_rating, args)\n",
    "    change_arr = change_csr.toarray()\n",
    "    path = get_path(args, id)\n",
    "    with open(path, \"w+\") as f:\n",
    "        for i, j in zipped_index:\n",
    "            print(i, j, change_arr[i, j], file=f, sep=',')\n",
    "\n",
    "\n",
    "def aggregate_process(edit, sorted_edges, train_csr, test_csr, args, old_pred, max_rating, min_rating, percent):\n",
    "    if args.implicit:\n",
    "        alpha = args.alpha\n",
    "    else:\n",
    "        alpha = 1\n",
    "    cut_pos = int(len(sorted_edges) * percent * 0.01)\n",
    "    base_arr = train_csr.todense()\n",
    "    for i, j, v in sorted_edges[:cut_pos]:\n",
    "        if edit == \"del\":\n",
    "            base_arr[i, j] = 0\n",
    "        elif edit == \"mod\":\n",
    "            base_arr[i, j] += v\n",
    "            base_arr[i, j] = min(max_rating, max(min_rating, base_arr[i, j]))\n",
    "    user_feature, item_feature = ALS(alpha * sparse.csr_matrix(base_arr), args, args.als_iter)\n",
    "    new_pred = np.dot(user_feature, item_feature.T)\n",
    "    if args.implicit:\n",
    "        aucs = roc_auc_with_t_test(new_pred, old_pred, test_csr)\n",
    "        mse = RMSE_weighted_with_t_test(new_pred, old_pred, alpha * test_csr)\n",
    "        precisions = precision_at_10_with_t_test(new_pred, old_pred, test_csr)\n",
    "    else:\n",
    "        test_csr_binarized = test_csr.copy()\n",
    "        test_csr_binarized[test_csr_binarized <= 3] = 0\n",
    "        test_csr_binarized[test_csr_binarized > 3] = 1\n",
    "        aucs = roc_auc_with_t_test(new_pred, old_pred, test_csr_binarized)\n",
    "        mse = RMSE_with_ttest(new_pred, old_pred, test_csr)\n",
    "        precisions = precision_at_10_with_t_test(new_pred, old_pred, test_csr_binarized)\n",
    "    return aucs, mse, precisions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:40.485556Z",
     "iopub.status.busy": "2023-03-27T23:51:40.485083Z",
     "iopub.status.idle": "2023-03-27T23:51:40.495233Z",
     "shell.execute_reply": "2023-03-27T23:51:40.493813Z",
     "shell.execute_reply.started": "2023-03-27T23:51:40.485495Z"
    }
   },
   "outputs": [],
   "source": [
    "class tempo:\n",
    "    def __init__(self, mode, implicit, negative):\n",
    "        self.dataset = 'movielens'\n",
    "        self.delim = '::'\n",
    "        self.fold = 4\n",
    "        self.factor = 10\n",
    "        self.lambda_u = 0.1\n",
    "        self.lambda_v = 0.1\n",
    "        self.als_iter = 15\n",
    "        self.debug_iter = 20\n",
    "        self.debug_lr = 0.05\n",
    "        self.retrain = 'full'\n",
    "        self.process = 4\n",
    "        self.mode = mode#'test'\n",
    "        self.implicit = implicit#'store_true'\n",
    "        self.alpha = 5\n",
    "        self.als_threads = 2\n",
    "        self.negative = negative#False   #change here to switch between negative weighting and confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:49.605673Z",
     "iopub.status.busy": "2023-03-27T23:51:49.605293Z",
     "iopub.status.idle": "2023-03-27T23:51:49.626804Z",
     "shell.execute_reply": "2023-03-27T23:51:49.625587Z",
     "shell.execute_reply.started": "2023-03-27T23:51:49.605636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 0 ns, total: 14 µs\n",
      "Wall time: 18.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "def run(args):\n",
    "    file_path = \"./data/\" + args.dataset + \".txt\"\n",
    "    if not os.path.exists(f\"./save/{args.dataset}\"):\n",
    "        os.mkdir(f\"./save/{args.dataset}\")\n",
    "\n",
    "    if args.mode == \"debug\":\n",
    "        train_csr, val_csr, test_csr, zipped_index, max_rating, min_rating = data_split(file_path, args)\n",
    "        if args.implicit:\n",
    "            max_rating *= args.alpha\n",
    "            min_rating = 0\n",
    "        fold_id = 0\n",
    "        for rnd in range((args.fold + args.process - 1) // args.process):\n",
    "            processes = []\n",
    "            for i in range(fold_id, fold_id + args.process):\n",
    "                process = Process(target=debug_process, args=(train_csr[i + 1], val_csr[i + 1], zipped_index[i + 1],\n",
    "                                                              max_rating, min_rating, i + 1, args))\n",
    "                processes.append(process)\n",
    "            for p in processes:\n",
    "                p.start()\n",
    "            for p in processes:\n",
    "                p.join()\n",
    "            fold_id += args.process\n",
    "    elif args.mode == \"test\":\n",
    "        del_rmse = []\n",
    "        rem_rmse = []\n",
    "        del_roc = []\n",
    "        rem_roc = []\n",
    "        del_prec = []\n",
    "        rem_prec = []\n",
    "        train_csr, val_csr, test_csr, zipped_index, max_rating, min_rating = data_split(file_path, args)\n",
    "        if args.implicit:\n",
    "            max_rating *= args.alpha\n",
    "            min_rating = 0\n",
    "            alpha = args.alpha\n",
    "        else:\n",
    "            alpha = 1\n",
    "        test_train_csr = sparse.csr_matrix(test_csr.shape)\n",
    "        for i in range(args.fold):\n",
    "            test_train_csr = test_train_csr + val_csr[i + 1]\n",
    "        user_feature, item_feature = ALS(alpha * test_train_csr, args, args.als_iter)\n",
    "        old_pred = np.dot(user_feature, item_feature.T)\n",
    "\n",
    "        edge_dict = dict()\n",
    "        for i in range(1, args.fold + 1):\n",
    "            path = get_path(args, i)\n",
    "            for line in open(path):\n",
    "                l = line.strip().split(',')\n",
    "                x = int(l[0])\n",
    "                y = int(l[1])\n",
    "                r = float(l[2])\n",
    "                if (x, y) not in edge_dict.keys():\n",
    "                    edge_dict[(x, y)] = r / (args.fold - 1)\n",
    "                else:\n",
    "                    if edge_dict[(x, y)] * r > 0:\n",
    "                        edge_dict[(x, y)] += r / (args.fold - 1)\n",
    "                    else:\n",
    "                        edge_dict[(x, y)] = 0\n",
    "        edges = [(key[0], key[1], values) for key, values in edge_dict.items()]\n",
    "        if args.implicit:\n",
    "            sorted_edges = sorted(edges, key=lambda _: _[2], reverse=False)\n",
    "        else:\n",
    "            sorted_edges = sorted(edges, key=lambda _: abs(_[2]), reverse=True)\n",
    "        for edit in [\"del\", \"mod\"]:\n",
    "            for percent in [0.1, 0.2, 0.5, 1, 2, 5, 10]:\n",
    "                aucs, mse, precisions = aggregate_process(edit, sorted_edges, test_train_csr,\n",
    "                                                          test_csr, args, old_pred,\n",
    "                                                          max_rating, min_rating, percent)\n",
    "                if edit =='del':\n",
    "                    del_rmse.append(mse[0])\n",
    "                    del_roc.append(aucs[0])\n",
    "                    del_prec.append(precisions[0])\n",
    "                else:\n",
    "                    rem_rmse.append(mse[0])\n",
    "                    rem_roc.append(aucs[0])\n",
    "                    rem_prec.append(precisions[0])\n",
    "                if args.implicit:\n",
    "                    print(f\"{edit} {percent}% training data, weighted rmse on test: {mse[1]} -> {mse[0]}, p_value: {mse[2]}\")\n",
    "                else:\n",
    "                    print(f\"{edit} {percent}% training data, rmse on test: {mse[1]} -> {mse[0]}, p_value: {mse[2]}\")\n",
    "                print(f\"{edit} {percent}% training data, aucs on test: {aucs[1]} -> {aucs[0]}, p_value: {aucs[2]}\")\n",
    "                print(f\"{edit} {percent}% training data, p@10 on test: {precisions[1]} -> {precisions[0]}, p_value: {precisions[2]}\")\n",
    "        explicit = {}\n",
    "        explicit['modify'] = {'rmse':rem_rmse, 'roc_auc':rem_roc, 'precision': rem_prec}\n",
    "        explicit['delete'] = {'rmse':del_rmse, 'roc_auc': del_roc, 'precision': del_prec}\n",
    "\n",
    "        tr = pd.DataFrame(explicit).transpose()\n",
    "        pos = pd.DataFrame({'rmse':tr.rmse.explode(), 'roc_auc':tr.roc_auc.explode(), 'precision':tr.precision.explode()})\n",
    "        modify_exp = pos[pos.index=='modify'].copy()\n",
    "        modify_exp['percentage'] = [0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
    "        cols = ['percentage', 'rmse', 'roc_auc', 'precision']\n",
    "        modify_exp = modify_exp[cols]\n",
    "        delete_exp = pos[pos.index=='delete'].copy()\n",
    "        delete_exp['percentage'] = [0.1, 0.2, 0.5, 1, 2, 5, 10]\n",
    "        cols = ['percentage', 'rmse', 'roc_auc', 'precision']\n",
    "        delete_exp = delete_exp[cols]\n",
    "        return modify_exp, delete_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = tempo(mode = 'debug', implicit = 'store_true', negative = False)\n",
    "run(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T23:51:49.629580Z",
     "iopub.status.busy": "2023-03-27T23:51:49.628303Z",
     "iopub.status.idle": "2023-03-28T00:47:33.621807Z",
     "shell.execute_reply": "2023-03-28T00:47:33.620510Z",
     "shell.execute_reply.started": "2023-03-27T23:51:49.629531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == \"\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/compressed.py:291: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using <= is inefficient, try using > instead.\n",
      "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del 0.1% training data, rmse on test: 0.913437474491702 -> 0.9073873138880987, p_value: 1.4588867653275616e-15\n",
      "del 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7723590040132347, p_value: 6.129901054179996e-47\n",
      "del 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5271617308085856\n",
      "del 0.2% training data, rmse on test: 0.913437474491702 -> 0.9033484900440794, p_value: 8.050089614980532e-23\n",
      "del 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7743213250567609, p_value: 8.555573053898794e-76\n",
      "del 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5637730167013997\n",
      "del 0.5% training data, rmse on test: 0.913437474491702 -> 0.898292760326892, p_value: 1.1773408796878419e-36\n",
      "del 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7801331147497039, p_value: 5.780865113039953e-157\n",
      "del 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00035078251484079874, p_value: 0.07069606522246734\n",
      "del 1% training data, rmse on test: 0.913437474491702 -> 0.8927170074662699, p_value: 9.710312533100217e-68\n",
      "del 1% training data, aucs on test: 0.7688397347194403 -> 0.7851139328218831, p_value: 4.085106492228866e-259\n",
      "del 1% training data, p@10 on test: 0.0001618996222342148 -> 0.00040474905558553697, p_value: 0.03893078117141074\n",
      "del 2% training data, rmse on test: 0.913437474491702 -> 0.8876276245194641, p_value: 6.296705493023683e-93\n",
      "del 2% training data, aucs on test: 0.7688397347194403 -> 0.7925921987584407, p_value: 0.0\n",
      "del 2% training data, p@10 on test: 0.0001618996222342148 -> 0.00048569886670264445, p_value: 0.01428617256250652\n",
      "del 5% training data, rmse on test: 0.913437474491702 -> 0.8856764963057757, p_value: 1.6931729682176503e-95\n",
      "del 5% training data, aucs on test: 0.7688397347194403 -> 0.8025626775794855, p_value: 0.0\n",
      "del 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0010523475445223963, p_value: 8.375339972336538e-07\n",
      "del 10% training data, rmse on test: 0.913437474491702 -> 0.8889812277014134, p_value: 2.136473307357492e-63\n",
      "del 10% training data, aucs on test: 0.7688397347194403 -> 0.8113526023186876, p_value: 0.0\n",
      "del 10% training data, p@10 on test: 0.0001618996222342148 -> 0.0012142471667566108, p_value: 2.3774319121737882e-08\n",
      "mod 0.1% training data, rmse on test: 0.913437474491702 -> 0.9052936159794959, p_value: 4.6656114789352225e-26\n",
      "mod 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7725780585060916, p_value: 6.7634807677642294e-77\n",
      "mod 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.0001618996222342148, p_value: 1.0\n",
      "mod 0.2% training data, rmse on test: 0.913437474491702 -> 0.9011219443120054, p_value: 4.068680574720488e-44\n",
      "mod 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7760425280262492, p_value: 3.0295433227650017e-173\n",
      "mod 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.0002698327037236913, p_value: 0.3173758126932722\n",
      "mod 0.5% training data, rmse on test: 0.913437474491702 -> 0.8943853375909748, p_value: 3.104308044669012e-78\n",
      "mod 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7818429313459041, p_value: 0.0\n",
      "mod 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00029681597409606046, p_value: 0.22530076890732273\n",
      "mod 1% training data, rmse on test: 0.913437474491702 -> 0.8880276777116627, p_value: 3.166269018230575e-126\n",
      "mod 1% training data, aucs on test: 0.7688397347194403 -> 0.7887205859788688, p_value: 0.0\n",
      "mod 1% training data, p@10 on test: 0.0001618996222342148 -> 0.0004587155963302752, p_value: 0.016357281407813413\n",
      "mod 2% training data, rmse on test: 0.913437474491702 -> 0.8812064870149748, p_value: 8.249180264722856e-174\n",
      "mod 2% training data, aucs on test: 0.7688397347194403 -> 0.7977500988702074, p_value: 0.0\n",
      "mod 2% training data, p@10 on test: 0.0001618996222342148 -> 0.0004856988667026444, p_value: 0.010496730115705693\n",
      "mod 5% training data, rmse on test: 0.913437474491702 -> 0.8735115114954207, p_value: 9.268353110158512e-256\n",
      "mod 5% training data, aucs on test: 0.7688397347194403 -> 0.8056921888980414, p_value: 0.0\n",
      "mod 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0027792768483540205, p_value: 3.9573738766261045e-19\n",
      "mod 10% training data, rmse on test: 0.913437474491702 -> 0.8695139528745185, p_value: 1.5706066008192407e-301\n",
      "mod 10% training data, aucs on test: 0.7688397347194403 -> 0.8102100454265159, p_value: 0.0\n",
      "mod 10% training data, p@10 on test: 0.0001618996222342148 -> 0.005774419859686995, p_value: 6.193606675786653e-36\n"
     ]
    }
   ],
   "source": [
    "args = tempo(mode = 'test', implicit='', negative = False)\n",
    "modify_exp, delete_exp = run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:47:33.625073Z",
     "iopub.status.busy": "2023-03-28T00:47:33.624158Z",
     "iopub.status.idle": "2023-03-28T00:47:33.651545Z",
     "shell.execute_reply": "2023-03-28T00:47:33.649897Z",
     "shell.execute_reply.started": "2023-03-28T00:47:33.625031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>rmse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.905294</td>\n",
       "      <td>0.772578</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.901122</td>\n",
       "      <td>0.776043</td>\n",
       "      <td>0.00027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.894385</td>\n",
       "      <td>0.781843</td>\n",
       "      <td>0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.888028</td>\n",
       "      <td>0.788721</td>\n",
       "      <td>0.000459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.881206</td>\n",
       "      <td>0.79775</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.873512</td>\n",
       "      <td>0.805692</td>\n",
       "      <td>0.002779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.869514</td>\n",
       "      <td>0.81021</td>\n",
       "      <td>0.005774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percentage      rmse   roc_auc precision\n",
       "modify         0.1  0.905294  0.772578  0.000162\n",
       "modify         0.2  0.901122  0.776043   0.00027\n",
       "modify         0.5  0.894385  0.781843  0.000297\n",
       "modify         1.0  0.888028  0.788721  0.000459\n",
       "modify         2.0  0.881206   0.79775  0.000486\n",
       "modify         5.0  0.873512  0.805692  0.002779\n",
       "modify        10.0  0.869514   0.81021  0.005774"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-28T00:52:13.006085Z",
     "iopub.status.busy": "2023-03-28T00:52:13.005561Z",
     "iopub.status.idle": "2023-03-28T00:52:13.019578Z",
     "shell.execute_reply": "2023-03-28T00:52:13.018612Z",
     "shell.execute_reply.started": "2023-03-28T00:52:13.006042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>rmse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.907387</td>\n",
       "      <td>0.772359</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.903348</td>\n",
       "      <td>0.774321</td>\n",
       "      <td>0.000216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.898293</td>\n",
       "      <td>0.780133</td>\n",
       "      <td>0.000351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892717</td>\n",
       "      <td>0.785114</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.887628</td>\n",
       "      <td>0.792592</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.885676</td>\n",
       "      <td>0.802563</td>\n",
       "      <td>0.001052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.888981</td>\n",
       "      <td>0.811353</td>\n",
       "      <td>0.001214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percentage      rmse   roc_auc precision\n",
       "delete         0.1  0.907387  0.772359  0.000216\n",
       "delete         0.2  0.903348  0.774321  0.000216\n",
       "delete         0.5  0.898293  0.780133  0.000351\n",
       "delete         1.0  0.892717  0.785114  0.000405\n",
       "delete         2.0  0.887628  0.792592  0.000486\n",
       "delete         5.0  0.885676  0.802563  0.001052\n",
       "delete        10.0  0.888981  0.811353  0.001214"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-26T20:26:52.763563Z",
     "iopub.status.busy": "2023-03-26T20:26:52.763054Z",
     "iopub.status.idle": "2023-03-26T22:32:36.852747Z",
     "shell.execute_reply": "2023-03-26T22:32:36.851079Z",
     "shell.execute_reply.started": "2023-03-26T20:26:52.763522Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == \"\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18249644353091043, p_value: 0.0\n",
      "del 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9266967009060519, p_value: 0.0001276966667295583\n",
      "del 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.1351259552787999, p_value: 0.6625835311614025\n",
      "del 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18241544387685954, p_value: 0.0\n",
      "del 0.2% training data, aucs on test: 0.9267385280885827 -> 0.926672111344329, p_value: 0.0002249629032130969\n",
      "del 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13453155958109259, p_value: 0.03523967584882992\n",
      "del 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18217356975189314, p_value: 0.0\n",
      "del 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9266177272628835, p_value: 4.315969578028615e-05\n",
      "del 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13424851401075574, p_value: 0.019068586819048775\n",
      "del 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18177593517801396, p_value: 0.0\n",
      "del 1% training data, aucs on test: 0.9267385280885827 -> 0.9265707862287809, p_value: 0.00034402828823171646\n",
      "del 1% training data, p@10 on test: 0.13523917350693462 -> 0.13405038211151996, p_value: 0.03073382833867656\n",
      "del 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.181000143937043, p_value: 0.0\n",
      "del 2% training data, aucs on test: 0.9267385280885827 -> 0.9264603107718078, p_value: 0.00020678016196438656\n",
      "del 2% training data, p@10 on test: 0.13523917350693462 -> 0.13393716388338522, p_value: 0.04733036169293658\n",
      "del 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.17875145221605454, p_value: 0.0\n",
      "del 5% training data, aucs on test: 0.9267385280885827 -> 0.9256196911180123, p_value: 2.3420484230056785e-08\n",
      "del 5% training data, p@10 on test: 0.13523917350693462 -> 0.1338522502122842, p_value: 0.177311549733292\n",
      "del 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17528054669258408, p_value: 0.0\n",
      "del 10% training data, aucs on test: 0.9267385280885827 -> 0.9238172008563675, p_value: 3.349805100382119e-19\n",
      "del 10% training data, p@10 on test: 0.13523917350693462 -> 0.1309368808378149, p_value: 0.0010105145642007865\n",
      "mod 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.182544781391595, p_value: 0.0\n",
      "mod 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9267262201584202, p_value: 0.0019259884118490913\n",
      "mod 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.13518256439286727, p_value: 0.7518788905799987\n",
      "mod 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18251332371423598, p_value: 0.0\n",
      "mod 0.2% training data, aucs on test: 0.9267385280885827 -> 0.9267210282850065, p_value: 0.008282475125275702\n",
      "mod 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13475799603736202, p_value: 0.02686256152369693\n",
      "mod 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.1824209898498937, p_value: 0.0\n",
      "mod 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9267108151447911, p_value: 0.010338179664326779\n",
      "mod 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13504104160769886, p_value: 0.4679987006986216\n",
      "mod 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18227210574734334, p_value: 0.0\n",
      "mod 1% training data, aucs on test: 0.9267385280885827 -> 0.9267045349987055, p_value: 0.044903878957030866\n",
      "mod 1% training data, p@10 on test: 0.13523917350693462 -> 0.13436173223889047, p_value: 0.008535600607931688\n",
      "mod 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18198646125061801, p_value: 0.0\n",
      "mod 2% training data, aucs on test: 0.9267385280885827 -> 0.9267115058386151, p_value: 0.2626517436105023\n",
      "mod 2% training data, p@10 on test: 0.13523917350693462 -> 0.1344749504670252, p_value: 0.054383938401998844\n",
      "mod 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18116896142239164, p_value: 0.0\n",
      "mod 5% training data, aucs on test: 0.9267385280885827 -> 0.9267349802381507, p_value: 0.9304145391326415\n",
      "mod 5% training data, p@10 on test: 0.13523917350693462 -> 0.134135295782621, p_value: 0.03467041707178722\n",
      "mod 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17987895625367242, p_value: 0.0\n",
      "mod 10% training data, aucs on test: 0.9267385280885827 -> 0.9267673876531942, p_value: 0.6169991931918938\n",
      "mod 10% training data, p@10 on test: 0.13523917350693462 -> 0.1343334276818568, p_value: 0.1621662144650468\n",
      "del 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18249644353091043, p_value: 0.0\n",
      "del 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9266967009060519, p_value: 0.0001276966667295583\n",
      "del 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.1351259552787999, p_value: 0.6625835311614025\n",
      "del 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18241544387685954, p_value: 0.0\n",
      "del 0.2% training data, aucs on test: 0.9267385280885827 -> 0.926672111344329, p_value: 0.0002249629032130969\n",
      "del 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13453155958109259, p_value: 0.03523967584882992\n",
      "del 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18217356975189314, p_value: 0.0\n",
      "del 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9266177272628835, p_value: 4.315969578028615e-05\n",
      "del 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13424851401075574, p_value: 0.019068586819048775\n",
      "del 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18177593517801396, p_value: 0.0\n",
      "del 1% training data, aucs on test: 0.9267385280885827 -> 0.9265707862287809, p_value: 0.00034402828823171646\n",
      "del 1% training data, p@10 on test: 0.13523917350693462 -> 0.13405038211151996, p_value: 0.03073382833867656\n",
      "del 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.181000143937043, p_value: 0.0\n",
      "del 2% training data, aucs on test: 0.9267385280885827 -> 0.9264603107718078, p_value: 0.00020678016196438656\n",
      "del 2% training data, p@10 on test: 0.13523917350693462 -> 0.13393716388338522, p_value: 0.04733036169293658\n",
      "del 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.17875145221605454, p_value: 0.0\n",
      "del 5% training data, aucs on test: 0.9267385280885827 -> 0.9256196911180123, p_value: 2.3420484230056785e-08\n",
      "del 5% training data, p@10 on test: 0.13523917350693462 -> 0.1338522502122842, p_value: 0.177311549733292\n",
      "del 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17528054669258408, p_value: 0.0\n",
      "del 10% training data, aucs on test: 0.9267385280885827 -> 0.9238172008563675, p_value: 3.349805100382119e-19\n",
      "del 10% training data, p@10 on test: 0.13523917350693462 -> 0.1309368808378149, p_value: 0.0010105145642007865\n",
      "mod 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.182544781391595, p_value: 0.0\n",
      "mod 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9267262201584202, p_value: 0.0019259884118490913\n",
      "mod 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.13518256439286727, p_value: 0.7518788905799987\n",
      "mod 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18251332371423598, p_value: 0.0\n",
      "mod 0.2% training data, aucs on test: 0.9267385280885827 -> 0.9267210282850065, p_value: 0.008282475125275702\n",
      "mod 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13475799603736202, p_value: 0.02686256152369693\n",
      "mod 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.1824209898498937, p_value: 0.0\n",
      "mod 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9267108151447911, p_value: 0.010338179664326779\n",
      "mod 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13504104160769886, p_value: 0.4679987006986216\n",
      "mod 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18227210574734334, p_value: 0.0\n",
      "mod 1% training data, aucs on test: 0.9267385280885827 -> 0.9267045349987055, p_value: 0.044903878957030866\n",
      "mod 1% training data, p@10 on test: 0.13523917350693462 -> 0.13436173223889047, p_value: 0.008535600607931688\n",
      "mod 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18198646125061801, p_value: 0.0\n",
      "mod 2% training data, aucs on test: 0.9267385280885827 -> 0.9267115058386151, p_value: 0.2626517436105023\n",
      "mod 2% training data, p@10 on test: 0.13523917350693462 -> 0.1344749504670252, p_value: 0.054383938401998844\n",
      "mod 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18116896142239164, p_value: 0.0\n",
      "mod 5% training data, aucs on test: 0.9267385280885827 -> 0.9267349802381507, p_value: 0.9304145391326415\n",
      "mod 5% training data, p@10 on test: 0.13523917350693462 -> 0.134135295782621, p_value: 0.03467041707178722\n",
      "mod 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17987895625367242, p_value: 0.0\n",
      "mod 10% training data, aucs on test: 0.9267385280885827 -> 0.9267673876531942, p_value: 0.6169991931918938\n",
      "mod 10% training data, p@10 on test: 0.13523917350693462 -> 0.1343334276818568, p_value: 0.1621662144650468\n"
     ]
    }
   ],
   "source": [
    "args = tempo(mode = 'test', implicit = 'store_true', negative = False)\n",
    "modify_pos, delete_pos = run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T18:39:36.081278Z",
     "iopub.status.busy": "2023-03-27T18:39:36.080768Z",
     "iopub.status.idle": "2023-03-27T21:48:57.914376Z",
     "shell.execute_reply": "2023-03-27T21:48:57.913363Z",
     "shell.execute_reply.started": "2023-03-27T18:39:36.081235Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if sys.path[0] == \"\":\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "del 0.1% training data, weighted rmse on test: 0.22937876966031367 -> 0.22918851646859728, p_value: 9.172257738508885e-149\n",
      "del 0.1% training data, aucs on test: 0.925282652862054 -> 0.9251733977402747, p_value: 0.3002032108743159\n",
      "del 0.1% training data, p@10 on test: 0.13470138692329467 -> 0.1356071327483725, p_value: 0.3351366276267669\n",
      "del 0.2% training data, weighted rmse on test: 0.22937876966031367 -> 0.22919547113560973, p_value: 5.2221957849643696e-136\n",
      "del 0.2% training data, aucs on test: 0.925282652862054 -> 0.9250716421787, p_value: 0.038785521765133044\n",
      "del 0.2% training data, p@10 on test: 0.13470138692329467 -> 0.13422020945372207, p_value: 0.6094581331873967\n",
      "del 0.5% training data, weighted rmse on test: 0.22937876966031367 -> 0.22882014853625174, p_value: 0.0\n",
      "del 0.5% training data, aucs on test: 0.925282652862054 -> 0.9252554912996659, p_value: 0.80160644342261\n",
      "del 0.5% training data, p@10 on test: 0.13470138692329467 -> 0.13458816869515994, p_value: 0.9034108218656115\n",
      "del 1% training data, weighted rmse on test: 0.22937876966031367 -> 0.22851107254720207, p_value: 0.0\n",
      "del 1% training data, aucs on test: 0.925282652862054 -> 0.9251914517115511, p_value: 0.3994514207639228\n",
      "del 1% training data, p@10 on test: 0.13470138692329467 -> 0.1343334276818568, p_value: 0.6988895687900004\n",
      "del 2% training data, weighted rmse on test: 0.22937876966031367 -> 0.2277491120780061, p_value: 0.0\n",
      "del 2% training data, aucs on test: 0.925282652862054 -> 0.9250309512573877, p_value: 0.042061384907926186\n",
      "del 2% training data, p@10 on test: 0.13470138692329467 -> 0.13484290970846308, p_value: 0.8857766662496611\n",
      "del 5% training data, weighted rmse on test: 0.22937876966031367 -> 0.22548570063949994, p_value: 0.0\n",
      "del 5% training data, aucs on test: 0.925282652862054 -> 0.923873048427941, p_value: 5.2821336320762994e-08\n",
      "del 5% training data, p@10 on test: 0.13470138692329467 -> 0.13492782337956413, p_value: 0.8622843010586292\n",
      "del 10% training data, weighted rmse on test: 0.22937876966031367 -> 0.22168715601131492, p_value: 0.0\n",
      "del 10% training data, aucs on test: 0.925282652862054 -> 0.9228417105251385, p_value: 1.1503580671124774e-13\n",
      "del 10% training data, p@10 on test: 0.13470138692329467 -> 0.13390885932635155, p_value: 0.5660790757571483\n",
      "mod 0.1% training data, weighted rmse on test: 0.22937876966031367 -> 0.22935850066193364, p_value: 0.005435009418556606\n",
      "mod 0.1% training data, aucs on test: 0.925282652862054 -> 0.9251982239437199, p_value: 0.4126007840116038\n",
      "mod 0.1% training data, p@10 on test: 0.13470138692329467 -> 0.1346164732521936, p_value: 0.9261459273207535\n",
      "mod 0.2% training data, weighted rmse on test: 0.22937876966031367 -> 0.22930326340854607, p_value: 5.324957103579647e-25\n",
      "mod 0.2% training data, aucs on test: 0.925282652862054 -> 0.9253054664988861, p_value: 0.8221907434904043\n",
      "mod 0.2% training data, p@10 on test: 0.13470138692329467 -> 0.13526747806396833, p_value: 0.5402399991518095\n",
      "mod 0.5% training data, weighted rmse on test: 0.22937876966031367 -> 0.229165154266553, p_value: 3.769600977193382e-187\n",
      "mod 0.5% training data, aucs on test: 0.925282652862054 -> 0.9251261862728917, p_value: 0.11390352810388714\n",
      "mod 0.5% training data, p@10 on test: 0.13470138692329467 -> 0.13359750919898106, p_value: 0.24160757380493106\n",
      "mod 1% training data, weighted rmse on test: 0.22937876966031367 -> 0.2290599331439326, p_value: 0.0\n",
      "mod 1% training data, aucs on test: 0.925282652862054 -> 0.9252948014288561, p_value: 0.9163069811945791\n",
      "mod 1% training data, p@10 on test: 0.13470138692329467 -> 0.1348146051514294, p_value: 0.9012139697367693\n",
      "mod 2% training data, weighted rmse on test: 0.22937876966031367 -> 0.2287440181039156, p_value: 0.0\n",
      "mod 2% training data, aucs on test: 0.925282652862054 -> 0.9251284831283285, p_value: 0.126748515115073\n",
      "mod 2% training data, p@10 on test: 0.13470138692329467 -> 0.13314463628644213, p_value: 0.09710820005992223\n",
      "mod 5% training data, weighted rmse on test: 0.22937876966031367 -> 0.22793207961914586, p_value: 0.0\n",
      "mod 5% training data, aucs on test: 0.925282652862054 -> 0.9252765593704482, p_value: 0.9528930044178182\n",
      "mod 5% training data, p@10 on test: 0.13470138692329467 -> 0.1343334276818568, p_value: 0.7001499784811172\n",
      "mod 10% training data, weighted rmse on test: 0.22937876966031367 -> 0.22670411943738417, p_value: 0.0\n",
      "mod 10% training data, aucs on test: 0.925282652862054 -> 0.925167429889253, p_value: 0.3835177054750186\n",
      "mod 10% training data, p@10 on test: 0.13470138692329467 -> 0.13475799603736205, p_value: 0.9575318676182065\n",
      "CPU times: user 5h 11min 55s, sys: 6h 21min 42s, total: 11h 33min 38s\n",
      "Wall time: 3h 9min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "args = tempo(mode = 'test', implicit = 'store_true', negative = True)\n",
    "modify_neg, delete_neg = run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T21:50:07.609941Z",
     "iopub.status.busy": "2023-03-27T21:50:07.609548Z",
     "iopub.status.idle": "2023-03-27T21:50:07.632803Z",
     "shell.execute_reply": "2023-03-27T21:50:07.631913Z",
     "shell.execute_reply.started": "2023-03-27T21:50:07.609907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>rmse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.229359</td>\n",
       "      <td>0.925198</td>\n",
       "      <td>0.134616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.229303</td>\n",
       "      <td>0.925305</td>\n",
       "      <td>0.135267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.229165</td>\n",
       "      <td>0.925126</td>\n",
       "      <td>0.133598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.22906</td>\n",
       "      <td>0.925295</td>\n",
       "      <td>0.134815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.228744</td>\n",
       "      <td>0.925128</td>\n",
       "      <td>0.133145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.227932</td>\n",
       "      <td>0.925277</td>\n",
       "      <td>0.134333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modify</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.226704</td>\n",
       "      <td>0.925167</td>\n",
       "      <td>0.134758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percentage      rmse   roc_auc precision\n",
       "modify         0.1  0.229359  0.925198  0.134616\n",
       "modify         0.2  0.229303  0.925305  0.135267\n",
       "modify         0.5  0.229165  0.925126  0.133598\n",
       "modify         1.0   0.22906  0.925295  0.134815\n",
       "modify         2.0  0.228744  0.925128  0.133145\n",
       "modify         5.0  0.227932  0.925277  0.134333\n",
       "modify        10.0  0.226704  0.925167  0.134758"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modify_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-27T21:50:08.380837Z",
     "iopub.status.busy": "2023-03-27T21:50:08.379606Z",
     "iopub.status.idle": "2023-03-27T21:50:08.392407Z",
     "shell.execute_reply": "2023-03-27T21:50:08.391545Z",
     "shell.execute_reply.started": "2023-03-27T21:50:08.380781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percentage</th>\n",
       "      <th>rmse</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.229189</td>\n",
       "      <td>0.925173</td>\n",
       "      <td>0.135607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.229195</td>\n",
       "      <td>0.925072</td>\n",
       "      <td>0.13422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.22882</td>\n",
       "      <td>0.925255</td>\n",
       "      <td>0.134588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228511</td>\n",
       "      <td>0.925191</td>\n",
       "      <td>0.134333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.227749</td>\n",
       "      <td>0.925031</td>\n",
       "      <td>0.134843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.225486</td>\n",
       "      <td>0.923873</td>\n",
       "      <td>0.134928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delete</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.221687</td>\n",
       "      <td>0.922842</td>\n",
       "      <td>0.133909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        percentage      rmse   roc_auc precision\n",
       "delete         0.1  0.229189  0.925173  0.135607\n",
       "delete         0.2  0.229195  0.925072   0.13422\n",
       "delete         0.5   0.22882  0.925255  0.134588\n",
       "delete         1.0  0.228511  0.925191  0.134333\n",
       "delete         2.0  0.227749  0.925031  0.134843\n",
       "delete         5.0  0.225486  0.923873  0.134928\n",
       "delete        10.0  0.221687  0.922842  0.133909"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
