{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom scipy import sparse\nfrom scipy.stats import ttest_rel\nfrom typing import Union, Tuple\n\n\ndef load_data(data_path, args):\n    raw_data = np.loadtxt(data_path, dtype=np.float, delimiter=args.delim, usecols=[0, 1, 2])\n    if args.implicit:\n        raw_data = raw_data[raw_data[:, 2] > 3]\n        raw_data[:, 2] = 1\n    users = list(set(raw_data[:, 0].astype(np.int)))\n    users.sort()\n    user_dict = {k: i for i, k in enumerate(users)}\n    items = list(set(raw_data[:, 1].astype(np.int)))\n    items.sort()\n    item_dict = {k: i for i, k in enumerate(items)}\n    for i in range(len(raw_data)):\n        raw_data[i, 0] = user_dict[raw_data[i, 0]]\n        raw_data[i, 1] = item_dict[raw_data[i, 1]]\n    return raw_data\n\n\ndef build_user_item_matrix(ratings, n_user, n_item):\n    data = ratings[:, 2]\n    row_index = ratings[:, 0]\n    col_index = ratings[:, 1]\n    shape = (n_user, n_item)\n    return sparse.csr_matrix((data, (row_index, col_index)), shape=shape)\n\n\ndef RMSE(estimation, truth):\n    truth_coo = truth.tocoo()\n    row_idx = truth_coo.row\n    col_idx = truth_coo.col\n    data = truth.data\n    pred = np.zeros(shape=data.shape)\n    for i in range(len(data)):\n        pred[i] = estimation[row_idx[i], col_idx[i]]\n    sse = np.sum(np.square(data - pred))\n    return np.sqrt(np.divide(sse, len(data)))\n\n\ndef RMSE_with_ttest(estimation, old_estimation, truth):\n    truth_coo = truth.tocoo()\n    row_idx = truth_coo.row\n    col_idx = truth_coo.col\n    data = truth_coo.data\n    pred_dis = np.zeros(shape=data.shape)\n    old_pred_dis = np.zeros(shape=data.shape)\n    for i in range(len(data)):\n        pred_dis[i] = abs(estimation[row_idx[i], col_idx[i]] - data[i])\n        old_pred_dis[i] = abs(old_estimation[row_idx[i], col_idx[i]] - data[i])\n    _, p_value = ttest_rel(pred_dis, old_pred_dis)\n    sse = np.sum(np.square(pred_dis))\n    sse_old = np.sum(np.square(old_pred_dis))\n    return np.sqrt(np.divide(sse, len(data))), np.sqrt(np.divide(sse_old, len(data))), p_value\n\n\ndef RMSE_weighted_with_t_test(estimation, old_estimation, val_confidence):\n    val_confidence_dense = val_confidence.toarray()\n    val_preference_dense = val_confidence_dense.copy()\n    val_preference_dense[val_preference_dense > 0] = 1\n    val_confidence_dense[val_confidence_dense == 0] = 1\n    old_error = val_confidence_dense * np.power(old_estimation - val_preference_dense, 2)\n    new_error = val_confidence_dense * np.power(estimation - val_preference_dense, 2)\n    _, p_val = ttest_rel(new_error.flatten(), old_error.flatten())\n    return np.sqrt(np.mean(new_error)), np.sqrt(np.mean(old_error)), p_val\n\n\ndef roc_auc_grouped(labels: np.ndarray,\n                    predictions: np.ndarray,\n                    group_ids: np.ndarray,\n                    return_aucs_list: bool = False) -> Union[Tuple[float, float, int], np.ndarray]:\n    # efficient implementation of grouped auc, see test_metrics.py for the correctness check\n\n    # l_max = labels.max()\n    # l_min = labels.min()\n    # logging.info(str(l_max) + ' ' + str(l_min))\n    # labels = (labels > l_max * 0.8).astype(int)\n    # sort group_ids, predictions and labels jointly by (group_id, prediction) key\n    indices = np.lexsort((predictions, group_ids))\n    group_ids = group_ids[indices]\n    labels = labels[indices]\n\n    # unique monotonic group_id\n    _, group_ids2 = np.unique(group_ids, return_inverse=True)\n    _, unique_counts = np.unique(group_ids, return_counts=True)\n\n    offsets = np.cumsum(unique_counts)\n    offsets = np.insert(offsets, 0, 0)\n\n    # number of negatives up to current element\n    nneg_thru = np.cumsum(1 - labels)\n\n    # number of negatives at the beginning of each group\n    group_starts = nneg_thru[offsets - 1]\n    group_starts[0] = 0\n\n    # number of negatives up to current element, restarting at each group\n    nneg = nneg_thru - group_starts[group_ids2]\n\n    # number of ordered pairs with the current element\n    inversions = (nneg * labels)\n\n    # number of negatives in each group\n    nneg_counts = nneg[offsets[1:] - 1]\n    npos_counts = unique_counts - nneg_counts\n\n    total_pairs = nneg_counts * npos_counts\n\n    # Number of ordered pairs in each group\n    ordered_pairs = np.bincount(group_ids2, weights=inversions)\n\n    aucs = ordered_pairs[total_pairs > 0] / total_pairs[total_pairs > 0]\n\n    if return_aucs_list:\n        return aucs\n    else:\n        return float(np.mean(aucs)), float(np.std(aucs)), int(np.sum(total_pairs > 0))\n\n\ndef roc_auc_with_t_test(estimation, old_estimation, truth):\n    user_ids = np.repeat(np.array(range(estimation.shape[1])), estimation.shape[0])\n    aucs_old = roc_auc_grouped(truth.toarray().flatten(), old_estimation.flatten(), user_ids, True)\n    aucs_new = roc_auc_grouped(truth.toarray().flatten(), estimation.flatten(), user_ids, True)\n    _, p_value = ttest_rel(aucs_new, aucs_old)\n    return np.mean(aucs_new), np.mean(aucs_old), p_value\n\n\ndef precision_at_k_grouped(labels: np.ndarray,\n                           predictions: np.ndarray,\n                           group_ids: np.ndarray,\n                           k: int = 10,\n                           return_precision_list: bool = False) -> Union[Tuple[float, float, int], np.ndarray]:\n    # efficient implementation of grouped precision@k, see test_metrics.py for the correctness check\n\n    # l_max = labels.max()\n    # l_min = labels.min()\n    # logging.info(str(l_max) + ' ' + str(l_min))\n    # labels = (labels > l_max * 0.8).astype(int)\n    # sort group_ids, predictions and labels jointly by (group_id, prediction) key\n    indices = np.lexsort((-predictions, group_ids))\n    group_ids = group_ids[indices]\n    labels = labels[indices]\n\n    # 0000, 1111, 222, 3, 555555\n\n    # unique monotonic group_id\n    _, group_ids2 = np.unique(group_ids, return_inverse=True)\n    _, unique_counts = np.unique(group_ids, return_counts=True)\n\n    offsets = np.cumsum(unique_counts)\n    offsets = np.insert(offsets, 0, 0)\n\n    # independent indexing in each group. e.g., [0, 1, 2, 0, 0, 0, 1, 2, 3, 4, 0, 1, 2]\n    group_indices = np.arange(group_ids.shape[0]) - offsets[group_ids2]\n\n    # number of points in each group or k\n    denominator = np.minimum(unique_counts[group_ids2], np.repeat(k, group_ids.shape[0]))\n    pr_at_k_vals = labels / denominator\n\n    pr_at_k_vals[group_indices >= k] = 0\n\n    group_pr_at_k = np.zeros(unique_counts.shape[0])\n    np.add.at(group_pr_at_k, group_ids2, pr_at_k_vals)\n\n    if return_precision_list:\n        return group_pr_at_k\n    else:\n        return float(np.mean(group_pr_at_k)), float(np.std(group_pr_at_k)), group_pr_at_k.shape[0]\n\n\ndef precision_at_10_with_t_test(estimation, old_estimation, truth):\n    user_ids = np.repeat(np.array(range(estimation.shape[1])), estimation.shape[0])\n    precisions_old = precision_at_k_grouped(truth.toarray().flatten(), old_estimation.flatten(), user_ids, 10, True)\n    precisions_new = precision_at_k_grouped(truth.toarray().flatten(), estimation.flatten(), user_ids, 10, True)\n    _, p_value = ttest_rel(precisions_new, precisions_old)\n    return np.mean(precisions_new), np.mean(precisions_old), p_value\n\n\ndef u_emb_d_c(lamb, C, R, v, user_ind, vvt):\n    # calculates derivatives of each component of the embedding of user 'user_ind'\n    # wrt each confidence value of the user\n    # return shape (embedding_dim, num non-zero elements in C[user_ind]\n\n    # see test_gradients.py for the correctness check\n    idxs = np.argwhere(C[user_ind]).flatten()\n    m_inv = np.linalg.inv(lamb * np.eye(v.shape[1], v.shape[1]) + vvt + \\\n                          np.einsum('i,ik->ik', C[user_ind, idxs] - R[user_ind, idxs], v[idxs]).T.dot(v[idxs]))\n    outer_products = np.einsum('ij,il->ijl', v[idxs], v[idxs])\n    m_inv_v_outer = np.einsum('ij,kj->ki', m_inv, v[idxs])\n    m_inv_dot_outer_products = np.einsum('ij,cjk->cik', m_inv, outer_products)\n    first_part = np.einsum('cji,i->cj', m_inv_dot_outer_products,\n                           m_inv.dot(np.einsum('i,ik->k', C[user_ind, idxs], v[idxs])))\n    return -first_part + m_inv_v_outer\n\n\ndef i_emb_d_c(lamb, C, R, u, item_ind, uut):\n    # calculates derivatives of each component of the embedding of item 'item_ind'\n    # wrt each confidence value of the item\n    # return shape (embedding_dim, num non-zero elements in C[:, item_ind]\n\n    # see test_gradients.py for the correctness check\n    idxs = np.argwhere(C[:, item_ind]).flatten()\n    m_inv = np.linalg.inv(lamb * np.eye(u.shape[1], u.shape[1]) + uut + \\\n                          np.einsum('i,ik->ik', C[idxs, item_ind] - R[idxs, item_ind], u[idxs]).T.dot(u[idxs]))\n    outer_products = np.einsum('ij,il->ijl', u[idxs], u[idxs])\n    m_inv_u_outer = np.einsum('ij,kj->ki', m_inv, u[idxs])\n    m_inv_dot_outer_products = np.einsum('ij,cjk->cik', m_inv, outer_products)\n    first_part = np.einsum('cji,i->cj', m_inv_dot_outer_products,\n                           m_inv.dot(np.einsum('i,ik->k', C[idxs, item_ind], u[idxs])))\n    return -first_part + m_inv_u_outer\n\n\ndef loss_d_emb(confidence_val: np.ndarray,\n               preference_val: np.ndarray,\n               pred_val: np.ndarray,\n               user_embeddings: np.ndarray,\n               item_embeddings: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    # calculates derivatives of loss on validation\n    # wrt user embeddings and item embeddings\n    # (note that following the original paper we ignore regularization here)\n\n    # see test_gradients.py for the correctness check\n    error_weights = confidence_val.copy()\n    error_weights[error_weights == 0] = 1\n    print(error_weights.shape, (pred_val-preference_val).shape)\n    diffs = 2 * np.multiply(error_weights, (pred_val - preference_val))\n    print(diffs.shape, item_embeddings.shape)\n    grad_r_user = diffs.dot(item_embeddings)\n    grad_r_item = diffs.T.dot(user_embeddings)\n    return grad_r_user, grad_r_item\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T14:04:28.621985Z","iopub.execute_input":"2023-03-26T14:04:28.622478Z","iopub.status.idle":"2023-03-26T14:04:29.161166Z","shell.execute_reply.started":"2023-03-26T14:04:28.622438Z","shell.execute_reply":"2023-03-26T14:04:29.159419Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#overload for fit method in als class for implementing negative weighting\n\nfrom implicit.utils import check_blas_config, check_random_state, nonzeros\nfrom implicit.cpu import _als\nfrom scipy.sparse import diags\nfrom scipy.sparse import csr_matrix\nimport scipy\nimport time\nimport logging\nfrom tqdm import tqdm\nfrom implicit.cpu.als import AlternatingLeastSquares as als\n\nlog = logging.getLogger(\"implicit\")\n\ndef check_csr(user_items):\n    if not isinstance(user_items, scipy.sparse.csr_matrix):\n        class_name = user_items.__class__.__name__\n        start = time.time()\n        user_items = user_items.tocsr()\n        warnings.warn(\n            f\"Method expects CSR input, and was passed {class_name} instead. \"\n            f\"Converting to CSR took {time.time() - start} seconds\",\n            ParameterWarning,\n        )\n    return user_items\n\nclass als_with_weights(als):\n    def __init__(\n        self,\n        factors=100,\n        regularization=0.01,\n        alpha=1.0, \n        weights = False,\n        dtype=np.float32,\n        use_native=True,\n        use_cg=True,\n        iterations=15,\n        calculate_training_loss=False,\n        num_threads=0,\n        random_state=None,\n    ):\n        super().__init__(\n            factors=factors,\n            regularization=regularization,\n            dtype=dtype,\n            use_native=use_native,\n            use_cg=use_cg,\n            iterations=iterations,\n            calculate_training_loss=calculate_training_loss,\n            num_threads=num_threads,\n            random_state=random_state,\n        )\n        self.alpha = alpha\n        self.weights = weights\n        \n        \n    def fit(self, user_items, show_progress=True, callback=None):\n        \"\"\"Factorizes the user_items matrix.\n        After calling this method, the members 'user_factors' and 'item_factors' will be\n        initialized with a latent factor model of the input data.\n        The user_items matrix does double duty here. It defines which items are liked by which\n        users (P_ui in the original paper), as well as how much confidence we have that the user\n        liked the item (C_ui).\n        The negative items are implicitly defined: This code assumes that positive items in the\n        user_items matrix means that the user liked the item. The negatives are left unset in this\n        sparse matrix: the library will assume that means Piu = 0 and Ciu = 1 for all these items.\n        Negative items can also be passed with a higher confidence value by passing a negative\n        value, indicating that the user disliked the item.\n        Parameters\n        ----------\n        user_items: csr_matrix\n            Matrix of confidences for the liked items. This matrix should be a csr_matrix where\n            the rows of the matrix are the users, the columns are the items liked that user,\n            and the value is the confidence that the user liked the item.\n        show_progress : bool, optional\n            Whether to show a progress bar during fitting\n        callback: Callable, optional\n            Callable function on each epoch with such arguments as epoch, elapsed time and progress\n        \"\"\"\n        # initialize the random state\n        random_state = check_random_state(self.random_state)\n\n        Cui = check_csr(user_items)\n        if Cui.dtype != np.float32:\n            Cui = Cui.astype(np.float32)\n\n        # Give the positive examples more weight if asked for\n#         if self.alpha != 1.0:\n#             Cui = self.alpha * Cui\n        if self.weights:   #negative weighting wiu = alpha*|Iu|\n            neg_Cui = csr_matrix(np.abs(Cui.astype(bool).toarray()-1))\n            sums = np.array(np.sum(Cui, axis = 1)).squeeze()      #weights can be really high, so just make them lower\n            neg_weights = diags(sums/np.mean(sums)/4)@neg_Cui\n            tri = csr_matrix(np.random.random_integers(0, 1, (neg_weights.shape)))\n            neg_weights = csr_matrix(np.where(tri.toarray()==1, neg_weights.toarray(), 0))\n            Cui = neg_weights+Cui\n        s = time.time()\n        Ciu = Cui.T.tocsr()\n        log.debug(\"Calculated transpose in %.3fs\", time.time() - s)\n\n        items, users = Ciu.shape\n\n        s = time.time()\n        # Initialize the variables randomly if they haven't already been set\n        if self.user_factors is None:\n            self.user_factors = random_state.rand(users, self.factors).astype(self.dtype) * 0.01\n        if self.item_factors is None:\n            self.item_factors = random_state.rand(items, self.factors).astype(self.dtype) * 0.01\n\n        log.debug(\"Initialized factors in %s\", time.time() - s)\n\n        # invalidate cached norms and squared factors\n        self._item_norms = self._user_norms = None\n        self._YtY = None\n        self._XtX = None\n        loss = None\n\n        solver = self.solver\n\n        log.debug(\"Running %i ALS iterations\", self.iterations)\n        with tqdm(total=self.iterations, disable=not show_progress) as progress:\n            # alternate between learning the user_factors from the item_factors and vice-versa\n            for iteration in range(self.iterations):\n                s = time.time()\n                solver(\n                    Cui,\n                    self.user_factors,\n                    self.item_factors,\n                    self.regularization,\n                    num_threads=self.num_threads,\n                )\n                solver(\n                    Ciu,\n                    self.item_factors,\n                    self.user_factors,\n                    self.regularization,\n                    num_threads=self.num_threads,\n                )\n                progress.update(1)\n\n                if self.calculate_training_loss:\n                    loss = _als.calculate_loss(\n                        Cui,\n                        self.user_factors,\n                        self.item_factors,\n                        self.regularization,\n                        num_threads=self.num_threads,\n                    )\n                    progress.set_postfix({\"loss\": loss})\n\n                    if not show_progress:\n                        log.info(\"loss %.4f\", loss)\n\n                # Backward compatibility\n                if not callback:\n                    callback = self.fit_callback\n                if callback:\n                    callback(iteration, time.time() - s, loss)\n\n        if self.calculate_training_loss:\n            log.info(\"Final training loss %.4f\", loss)\n\n        self._check_fit_errors()","metadata":{"execution":{"iopub.status.busy":"2023-03-26T14:04:29.163978Z","iopub.execute_input":"2023-03-26T14:04:29.164643Z","iopub.status.idle":"2023-03-26T14:04:29.473036Z","shell.execute_reply.started":"2023-03-26T14:04:29.164597Z","shell.execute_reply":"2023-03-26T14:04:29.471312Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/implicit/gpu/__init__.py:14: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: CUDA driver version is insufficient for CUDA runtime version (/home/conda/feedstock_root/build_artifacts/implicit_1643471602441/work/./implicit/gpu/utils.h:71)'\n  f\"CUDA extension is built, but disabling GPU support because of '{e}'\",\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport os\nfrom copy import deepcopy\nfrom multiprocessing import Process\nfrom numpy.linalg import inv\nfrom scipy import sparse\nimport logging\nimport implicit\n\nlogging.basicConfig()\nlogging.getLogger().setLevel(logging.INFO)\n\n\n\ndef partition(ratings, seed, fold):\n    np.random.RandomState(seed).shuffle(ratings)\n    test_size = int(0.2 * ratings.shape[0])\n\n    test_data = ratings[:test_size]\n    val_data = dict()\n    fold_size = (ratings.shape[0] - test_size) // fold\n    for i in range(fold - 1):\n        val_data[i + 1] = ratings[test_size + i * fold_size: test_size + (i + 1) * fold_size]\n    val_data[fold] = ratings[test_size + (fold - 1) * fold_size:]\n    return val_data, test_data\n\n\ndef data_split(data_path, args):\n    fold = args.fold\n    ratings = load_data(data_path, args)\n    n_users = int(max(ratings[:, 0]) + 1)\n    n_items = int(max(ratings[:, 1]) + 1)\n    max_rating = max(ratings[:, 2])\n    min_rating = min(ratings[:, 2])\n\n    val_data, test_data = partition(ratings, 0, fold)\n\n    lambda_dict = dict()\n    for i in range(fold):\n        if i == 0:\n            lambda_dict[1] = deepcopy(val_data[2])\n            for j in range(3, fold + 1):\n                lambda_dict[1] = np.vstack((lambda_dict[1], val_data[j]))\n        else:\n            lambda_dict[i + 1] = deepcopy(val_data[1])\n            for j in range(2, i + 1):\n                lambda_dict[i + 1] = np.vstack((lambda_dict[i + 1], val_data[j]))\n            for j in range(i + 2, fold + 1):\n                lambda_dict[i + 1] = np.vstack((lambda_dict[i + 1], val_data[j]))\n\n    zipped_index_dict = dict()\n    for i in range(fold):\n        zipped_index_dict[i + 1] = [(int(_[0]), int(_[1])) for _ in lambda_dict[i + 1]]\n\n    train_csr_dict = dict()\n    for i in range(fold):\n        train_csr_dict[i + 1] = build_user_item_matrix(lambda_dict[i + 1], n_users, n_items)\n    val_csr_dict = dict()\n    for i in range(fold):\n        val_csr_dict[i + 1] = build_user_item_matrix(val_data[i + 1], n_users, n_items)\n    test_csr = build_user_item_matrix(test_data, n_users, n_items)\n    return train_csr_dict, val_csr_dict, test_csr, zipped_index_dict, max_rating, min_rating\n\n\ndef cf_ridge_regression(csr_matrix, reg_lambda, fixed_feature, update_feature):\n    n_feature = fixed_feature.shape[1]\n    for i in range(csr_matrix.shape[0]):\n        _, idx = csr_matrix[i, :].nonzero()\n        valid_feature = fixed_feature.take(idx, axis=0)\n        ratings = csr_matrix[i, idx].todense()\n        A_i = np.dot(valid_feature.T, valid_feature) + reg_lambda * np.eye(n_feature)\n        V_i = np.dot(valid_feature.T, ratings.T)\n        update_feature[i, :] = np.squeeze(np.dot(inv(A_i), V_i))\n\n\ndef ALS(train_csr, args, n_iters, init_user_features=None, init_item_features=None):\n    if args.implicit:\n        logging.info('Implicit ALS, alpha {} max rating {}'.format(args.alpha, train_csr.data.max()))\n        model = als_with_weights(factors=args.factor, iterations=n_iters, weights = args.negative, #num_threads=args.als_threads,\n                                                     regularization=max(args.lambda_u, args.lambda_v),\n                                                     random_state=0)\n        model.fit(train_csr, show_progress=False)\n        return model.user_factors, model.item_factors\n    else:\n        user_features = 0.1 * np.random.RandomState(seed=0).rand(train_csr.shape[0], args.factor)\n        item_features = 0.1 * np.random.RandomState(seed=0).rand(train_csr.shape[1], args.factor)\n        if init_user_features is not None:\n            user_features = init_user_features\n        if init_item_features is not None:\n            item_features = init_item_features\n        train_csr_transpose = train_csr.T.tocsr()\n        for iteration in range(n_iters):\n            logging.info('Explicit ALS iteration {}'.format(iteration))\n            cf_ridge_regression(train_csr, args.lambda_u, item_features, user_features)\n            cf_ridge_regression(train_csr_transpose, args.lambda_v, user_features, item_features)\n        return user_features, item_features\n\n\ndef grad_calc(train_csr, val_csr, zipped_index, user_features, item_features, args):\n    n_users = train_csr.shape[0]\n    n_items = train_csr.shape[1]\n\n    grad_r_user = np.zeros(shape=user_features.shape, dtype=np.float)\n    grad_r_item = np.zeros(shape=item_features.shape, dtype=np.float)\n\n    val_coo = val_csr.tocoo()\n    pred_val = np.dot(user_features, item_features.T)\n    for i, j, v in zip(val_coo.row, val_coo.col, val_coo.data):\n        loss = 2 * (pred_val[i, j] - v)\n        grad_r_user[i] += loss * item_features[j]\n        grad_r_item[j] += loss * user_features[i]\n\n    grad_user_m = np.zeros(shape=(train_csr.nnz, args.factor))\n    grad_user_dict = {}\n    cnt = 0\n    for i in range(n_users):\n        _, item_idx = train_csr[i, :].nonzero()\n        item_feat = item_features.take(item_idx, axis=0)\n        A = np.eye(args.factor, dtype=np.float) * args.lambda_u + np.dot(item_feat.T, item_feat)\n        grad_user_m_i = np.dot(item_features, inv(A))\n        for i_idx in item_idx:\n            tup = (i, i_idx)\n            grad_user_dict[tup] = cnt\n            grad_user_m[cnt] = grad_user_m_i[i_idx][:]\n            cnt += 1\n\n    train_csc = train_csr.tocsc()\n    grad_item_m = np.zeros(shape=(train_csc.nnz, args.factor))\n    grad_item_dict = {}\n    cnt = 0\n    for i in range(n_items):\n        user_idx, _ = train_csc[:, i].nonzero()\n        user_feat = user_features.take(user_idx, axis=0)\n        A = np.eye(args.factor, dtype=np.float) * args.lambda_v + np.dot(user_feat.T, user_feat)\n        grad_item_m_i = np.dot(user_features, inv(A))\n        for u_idx in user_idx:\n            tup = (i, u_idx)\n            grad_item_dict[tup] = cnt\n            grad_item_m[cnt] = grad_item_m_i[u_idx][:]\n            cnt += 1\n\n    row = [i for i, j in zipped_index]\n    col = [j for i, j in zipped_index]\n    data = [(np.dot(grad_r_user[i], grad_user_m[grad_user_dict[(i, j)]].T)\n             + np.dot(grad_r_item[j], grad_item_m[grad_item_dict[(j, i)]].T))\n            for i, j in zipped_index]\n    return sparse.coo_matrix((data, (row, col)), shape=(n_users, n_items)).tocsr()\n\n\ndef grad_calc_implicit(train_csr, val_csr, zipped_index, user_features, item_features, args):\n    n_users = train_csr.shape[0]\n    n_items = train_csr.shape[1]\n\n    pred_val = np.dot(user_features, item_features.T)\n    confidence_val_dense = val_csr.toarray()\n    confidence_train_dense = train_csr.toarray()\n    preference_val_dense = confidence_val_dense.copy()\n    preference_val_dense[preference_val_dense > 0] = 1\n    preference_train_dense = confidence_train_dense.copy()\n    preference_train_dense[preference_train_dense > 0] = 1\n    grad_r_user, grad_r_item = loss_d_emb(confidence_val_dense,\n                                          preference_val_dense,\n                                          pred_val,\n                                          user_features,\n                                          item_features)\n\n    grad_user_m = np.zeros(shape=(train_csr.nnz, args.factor))\n    grad_user_dict = {}\n    cnt = 0\n    VVT = np.dot(item_features.T, item_features)\n    UUT = np.dot(user_features.T, user_features)\n\n    for i in range(n_users):\n        _, item_idx = train_csr[i, :].nonzero()\n        grad_user_m_i = u_emb_d_c(args.lambda_u, confidence_train_dense, preference_train_dense, item_features, i, VVT)\n        for item_num, i_idx in enumerate(item_idx):\n            tup = (i, i_idx)\n            grad_user_dict[tup] = cnt\n            grad_user_m[cnt] = grad_user_m_i[item_num][:]\n            cnt += 1\n\n    train_csc = train_csr.tocsc()\n    grad_item_m = np.zeros(shape=(train_csc.nnz, args.factor))\n    grad_item_dict = {}\n    cnt = 0\n    for i in range(n_items):\n        user_idx, _ = train_csc[:, i].nonzero()\n        grad_item_m_i = i_emb_d_c(args.lambda_v, confidence_train_dense, preference_train_dense, user_features, i, UUT)\n        for user_num, u_idx in enumerate(user_idx):\n            tup = (i, u_idx)\n            grad_item_dict[tup] = cnt\n            grad_item_m[cnt] = grad_item_m_i[user_num][:]\n            cnt += 1\n\n    row = [i for i, j in zipped_index]\n    col = [j for i, j in zipped_index]\n    data = [(np.dot(grad_r_user[i], grad_user_m[grad_user_dict[(i, j)]].T)\n             + np.dot(grad_r_item[j], grad_item_m[grad_item_dict[(j, i)]].T))\n            for i, j in zipped_index]\n    return sparse.coo_matrix((data, (row, col)), shape=(n_users, n_items)).tocsr()\n\n\ndef grad_update_loop(train_csr, val_csr, zipped_index, max_rating, min_rating, args):\n    user_feature, item_feature = ALS(train_csr, args, args.als_iter)\n\n    A_i = train_csr\n    C_i = sparse.csr_matrix(train_csr.shape)\n    user_feature_i = user_feature\n    item_feature_i = item_feature\n    for i in range(args.debug_iter):\n        if args.implicit:\n            gradients = grad_calc_implicit(A_i, val_csr, zipped_index, user_feature_i, item_feature_i, args)\n        else:\n            gradients = grad_calc(A_i, val_csr, zipped_index, user_feature_i, item_feature_i, args)\n        A_i = A_i - gradients * args.debug_lr\n        for _ in range(A_i.nnz):\n            A_i.data[_] = min(max_rating, max(min_rating, A_i.data[_]))\n        logging.info(\"A_i mean {}, min {}, max {}\".format(A_i.data.mean(), A_i.data.min(), A_i.data.max()))\n        if args.retrain == \"full\":\n            user_feature_i, item_feature_i = ALS(A_i, args, args.als_iter)\n        if args.retrain == \"inc\":\n            user_feature_i, item_feature_i = ALS(A_i, args, 1, user_feature_i, item_feature_i)\n        C_i = A_i - train_csr\n    return C_i\n\n\ndef get_path(args, part_id):\n    path = f\"./save/{args.dataset}/f{args.fold}_m{args.debug_iter}_lr{args.debug_lr}_part{part_id}_{args.retrain}\"\n    if args.implicit:\n        path += '_implicit'\n    return path + '.txt'\n\n\ndef debug_process(train_csr, val_csr, zipped_index, max_rating, min_rating, id, args):\n    if args.implicit:\n        alpha = args.alpha\n    else:\n        alpha = 1\n    change_csr = grad_update_loop(alpha * train_csr, alpha * val_csr, zipped_index, max_rating, min_rating, args)\n    change_arr = change_csr.toarray()\n    path = get_path(args, id)\n    with open(path, \"w+\") as f:\n        for i, j in zipped_index:\n            print(i, j, change_arr[i, j], file=f, sep=',')\n\n\ndef aggregate_process(edit, sorted_edges, train_csr, test_csr, args, old_pred, max_rating, min_rating, percent):\n    if args.implicit:\n        alpha = args.alpha\n    else:\n        alpha = 1\n    cut_pos = int(len(sorted_edges) * percent * 0.01)\n    base_arr = train_csr.todense()\n    for i, j, v in sorted_edges[:cut_pos]:\n        if edit == \"del\":\n            base_arr[i, j] = 0\n        elif edit == \"mod\":\n            base_arr[i, j] += v\n            base_arr[i, j] = min(max_rating, max(min_rating, base_arr[i, j]))\n    user_feature, item_feature = ALS(alpha * sparse.csr_matrix(base_arr), args, args.als_iter)\n    new_pred = np.dot(user_feature, item_feature.T)\n    if args.implicit:\n        aucs = roc_auc_with_t_test(new_pred, old_pred, test_csr)\n        mse = RMSE_weighted_with_t_test(new_pred, old_pred, alpha * test_csr)\n        precisions = precision_at_10_with_t_test(new_pred, old_pred, test_csr)\n    else:\n        test_csr_binarized = test_csr.copy()\n        test_csr_binarized[test_csr_binarized <= 3] = 0\n        test_csr_binarized[test_csr_binarized > 3] = 1\n        aucs = roc_auc_with_t_test(new_pred, old_pred, test_csr_binarized)\n        mse = RMSE_with_ttest(new_pred, old_pred, test_csr)\n        precisions = precision_at_10_with_t_test(new_pred, old_pred, test_csr_binarized)\n    return aucs, mse, precisions\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T14:04:29.540798Z","iopub.execute_input":"2023-03-26T14:04:29.541279Z","iopub.status.idle":"2023-03-26T14:04:29.612666Z","shell.execute_reply.started":"2023-03-26T14:04:29.541239Z","shell.execute_reply":"2023-03-26T14:04:29.610429Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class tempo:\n    def __init__(self, mode, implicit, negative):\n        self.dataset = 'movielens'\n        self.delim = '::'\n        self.fold = 4\n        self.factor = 10\n        self.lambda_u = 0.1\n        self.lambda_v = 0.1\n        self.als_iter = 15\n        self.debug_iter = 20\n        self.debug_lr = 0.05\n        self.retrain = 'full'\n        self.process = 4\n        self.mode = mode#'test'\n        self.implicit = implicit#'store_true'\n        self.alpha = 5\n        self.als_threads = 2\n        self.negative = negative#False   #change here to switch between negative weighting and confidence","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:26:18.016102Z","iopub.execute_input":"2023-03-26T20:26:18.016631Z","iopub.status.idle":"2023-03-26T20:26:18.026391Z","shell.execute_reply.started":"2023-03-26T20:26:18.016589Z","shell.execute_reply":"2023-03-26T20:26:18.024355Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"%%time\nimport pandas as pd\ndef run(args):\n    file_path = \"./data/\" + args.dataset + \".txt\"\n    if not os.path.exists(f\"./save/{args.dataset}\"):\n        os.mkdir(f\"./save/{args.dataset}\")\n\n    if args.mode == \"debug\":\n        train_csr, val_csr, test_csr, zipped_index, max_rating, min_rating = data_split(file_path, args)\n        if args.implicit:\n            max_rating *= args.alpha\n            min_rating = 0\n        fold_id = 0\n        for rnd in range((args.fold + args.process - 1) // args.process):\n            processes = []\n            for i in range(fold_id, fold_id + args.process):\n                process = Process(target=debug_process, args=(train_csr[i + 1], val_csr[i + 1], zipped_index[i + 1],\n                                                              max_rating, min_rating, i + 1, args))\n                processes.append(process)\n            for p in processes:\n                p.start()\n            for p in processes:\n                p.join()\n            fold_id += args.process\n    elif args.mode == \"test\":\n        del_rmse = []\n        rem_rmse = []\n        del_roc = []\n        rem_roc = []\n        del_prec = []\n        rem_prec = []\n        train_csr, val_csr, test_csr, zipped_index, max_rating, min_rating = data_split(file_path, args)\n        if args.implicit:\n            max_rating *= args.alpha\n            min_rating = 0\n            alpha = args.alpha\n        else:\n            alpha = 1\n        test_train_csr = sparse.csr_matrix(test_csr.shape)\n        for i in range(args.fold):\n            test_train_csr = test_train_csr + val_csr[i + 1]\n        user_feature, item_feature = ALS(alpha * test_train_csr, args, args.als_iter)\n        old_pred = np.dot(user_feature, item_feature.T)\n\n        edge_dict = dict()\n        for i in range(1, args.fold + 1):\n            path = get_path(args, i)\n            for line in open(path):\n                l = line.strip().split(',')\n                x = int(l[0])\n                y = int(l[1])\n                r = float(l[2])\n                if (x, y) not in edge_dict.keys():\n                    edge_dict[(x, y)] = r / (args.fold - 1)\n                else:\n                    if edge_dict[(x, y)] * r > 0:\n                        edge_dict[(x, y)] += r / (args.fold - 1)\n                    else:\n                        edge_dict[(x, y)] = 0\n        edges = [(key[0], key[1], values) for key, values in edge_dict.items()]\n        if args.implicit:\n            sorted_edges = sorted(edges, key=lambda _: _[2], reverse=False)\n        else:\n            sorted_edges = sorted(edges, key=lambda _: abs(_[2]), reverse=True)\n        for edit in [\"del\", \"mod\"]:\n            for percent in [0.1, 0.2, 0.5, 1, 2, 5, 10]:\n                aucs, mse, precisions = aggregate_process(edit, sorted_edges, test_train_csr,\n                                                          test_csr, args, old_pred,\n                                                          max_rating, min_rating, percent)\n                if edit =='del':\n                    del_rmse.append(mse[0])\n                    del_roc.append(aucs[0])\n                    del_prec.append(precisions[0])\n                else:\n                    rem_rmse.append(mse[0])\n                    rem_roc.append(aucs[0])\n                    rem_prec.append(precisions[0])\n                if args.implicit:\n                    print(f\"{edit} {percent}% training data, weighted rmse on test: {mse[1]} -> {mse[0]}, p_value: {mse[2]}\")\n                else:\n                    print(f\"{edit} {percent}% training data, rmse on test: {mse[1]} -> {mse[0]}, p_value: {mse[2]}\")\n                print(f\"{edit} {percent}% training data, aucs on test: {aucs[1]} -> {aucs[0]}, p_value: {aucs[2]}\")\n                print(f\"{edit} {percent}% training data, p@10 on test: {precisions[1]} -> {precisions[0]}, p_value: {precisions[2]}\")\n        explicit = {}\n        explicit['modify'] = {'rmse':rem_rmse, 'roc_auc':rem_roc, 'precision': rem_prec}\n        explicit['delete'] = {'rmse':del_rmse, 'roc_auc': del_roc, 'precision': del_prec}\n\n        tr = pd.DataFrame(explicit).transpose()\n        pos = pd.DataFrame({'rmse':tr.rmse.explode(), 'roc_auc':tr.roc_auc.explode(), 'precision':tr.precision.explode()})\n        modify_exp = pos[pos.index=='modify'].copy()\n        modify_exp['percentage'] = [0.1, 0.2, 0.5, 1, 2, 5, 10]\n        cols = ['percentage', 'rmse', 'roc_auc', 'precision']\n        modify_exp = modify_exp[cols]\n        delete_exp = pos[pos.index=='delete'].copy()\n        delete_exp['percentage'] = [0.1, 0.2, 0.5, 1, 2, 5, 10]\n        cols = ['percentage', 'rmse', 'roc_auc', 'precision']\n        delete_exp = delete_exp[cols]\n        return modify_exp, delete_exp","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:25:53.603072Z","iopub.execute_input":"2023-03-26T20:25:53.603586Z","iopub.status.idle":"2023-03-26T20:25:53.628016Z","shell.execute_reply.started":"2023-03-26T20:25:53.603538Z","shell.execute_reply":"2023-03-26T20:25:53.626281Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"CPU times: user 10 µs, sys: 11 µs, total: 21 µs\nWall time: 26.2 µs\n","output_type":"stream"}]},{"cell_type":"code","source":"args = tempo(mode = 'debug', implicit = 'store_true', negative = False)\nrun(args)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = tempo(mode = 'test', implicit='', negative = False)\nmodify_exp, delete_exp = run(args)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:57:23.468790Z","iopub.execute_input":"2023-03-26T22:57:23.469195Z","iopub.status.idle":"2023-03-27T00:17:26.657641Z","shell.execute_reply.started":"2023-03-26T22:57:23.469160Z","shell.execute_reply":"2023-03-27T00:17:26.656429Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  if sys.path[0] == \"\":\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  from ipykernel import kernelapp as app\n/opt/conda/lib/python3.7/site-packages/scipy/sparse/compressed.py:291: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using <= is inefficient, try using > instead.\n  warn(bad_scalar_msg, SparseEfficiencyWarning)\n/opt/conda/lib/python3.7/site-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n  self._set_arrayXarray(i, j, x)\n","output_type":"stream"},{"name":"stdout","text":"del 0.1% training data, rmse on test: 0.913437474491702 -> 0.9073873138880987, p_value: 1.4588867653275616e-15\ndel 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7723590040132347, p_value: 6.129901054179996e-47\ndel 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5271617308085856\ndel 0.2% training data, rmse on test: 0.913437474491702 -> 0.9033484900440794, p_value: 8.050089614980532e-23\ndel 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7743213250567609, p_value: 8.555573053898794e-76\ndel 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5637730167013997\ndel 0.5% training data, rmse on test: 0.913437474491702 -> 0.898292760326892, p_value: 1.1773408796878419e-36\ndel 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7801331147497039, p_value: 5.780865113039953e-157\ndel 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00035078251484079874, p_value: 0.07069606522246734\ndel 1% training data, rmse on test: 0.913437474491702 -> 0.8927170074662699, p_value: 9.710312533100217e-68\ndel 1% training data, aucs on test: 0.7688397347194403 -> 0.7851139328218831, p_value: 4.085106492228866e-259\ndel 1% training data, p@10 on test: 0.0001618996222342148 -> 0.00040474905558553697, p_value: 0.03893078117141074\ndel 2% training data, rmse on test: 0.913437474491702 -> 0.8876276245194641, p_value: 6.296705493023683e-93\ndel 2% training data, aucs on test: 0.7688397347194403 -> 0.7925921987584407, p_value: 0.0\ndel 2% training data, p@10 on test: 0.0001618996222342148 -> 0.00048569886670264445, p_value: 0.01428617256250652\ndel 5% training data, rmse on test: 0.913437474491702 -> 0.8856764963057757, p_value: 1.6931729682176503e-95\ndel 5% training data, aucs on test: 0.7688397347194403 -> 0.8025626775794855, p_value: 0.0\ndel 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0010523475445223963, p_value: 8.375339972336538e-07\ndel 10% training data, rmse on test: 0.913437474491702 -> 0.8889812277014134, p_value: 2.136473307357492e-63\ndel 10% training data, aucs on test: 0.7688397347194403 -> 0.8113526023186876, p_value: 0.0\ndel 10% training data, p@10 on test: 0.0001618996222342148 -> 0.0012142471667566108, p_value: 2.3774319121737882e-08\nmod 0.1% training data, rmse on test: 0.913437474491702 -> 0.9052936159794959, p_value: 4.6656114789352225e-26\nmod 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7725780585060916, p_value: 6.7634807677642294e-77\nmod 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.0001618996222342148, p_value: 1.0\nmod 0.2% training data, rmse on test: 0.913437474491702 -> 0.9011219443120054, p_value: 4.068680574720488e-44\nmod 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7760425280262492, p_value: 3.0295433227650017e-173\nmod 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.0002698327037236913, p_value: 0.3173758126932722\nmod 0.5% training data, rmse on test: 0.913437474491702 -> 0.8943853375909748, p_value: 3.104308044669012e-78\nmod 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7818429313459041, p_value: 0.0\nmod 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00029681597409606046, p_value: 0.22530076890732273\nmod 1% training data, rmse on test: 0.913437474491702 -> 0.8880276777116627, p_value: 3.166269018230575e-126\nmod 1% training data, aucs on test: 0.7688397347194403 -> 0.7887205859788688, p_value: 0.0\nmod 1% training data, p@10 on test: 0.0001618996222342148 -> 0.0004587155963302752, p_value: 0.016357281407813413\nmod 2% training data, rmse on test: 0.913437474491702 -> 0.8812064870149748, p_value: 8.249180264722856e-174\nmod 2% training data, aucs on test: 0.7688397347194403 -> 0.7977500988702074, p_value: 0.0\nmod 2% training data, p@10 on test: 0.0001618996222342148 -> 0.0004856988667026444, p_value: 0.010496730115705693\nmod 5% training data, rmse on test: 0.913437474491702 -> 0.8735115114954207, p_value: 9.268353110158512e-256\nmod 5% training data, aucs on test: 0.7688397347194403 -> 0.8056921888980414, p_value: 0.0\nmod 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0027792768483540205, p_value: 3.9573738766261045e-19\nmod 10% training data, rmse on test: 0.913437474491702 -> 0.8695139528745185, p_value: 1.5706066008192407e-301\nmod 10% training data, aucs on test: 0.7688397347194403 -> 0.8102100454265159, p_value: 0.0\nmod 10% training data, p@10 on test: 0.0001618996222342148 -> 0.005774419859686995, p_value: 6.193606675786653e-36\ndel 0.1% training data, rmse on test: 0.913437474491702 -> 0.9073873138880987, p_value: 1.4588867653275616e-15\ndel 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7723590040132347, p_value: 6.129901054179996e-47\ndel 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5271617308085856\ndel 0.2% training data, rmse on test: 0.913437474491702 -> 0.9033484900440794, p_value: 8.050089614980532e-23\ndel 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7743213250567609, p_value: 8.555573053898794e-76\ndel 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.00021586616297895306, p_value: 0.5637730167013997\ndel 0.5% training data, rmse on test: 0.913437474491702 -> 0.898292760326892, p_value: 1.1773408796878419e-36\ndel 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7801331147497039, p_value: 5.780865113039953e-157\ndel 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00035078251484079874, p_value: 0.07069606522246734\ndel 1% training data, rmse on test: 0.913437474491702 -> 0.8927170074662699, p_value: 9.710312533100217e-68\ndel 1% training data, aucs on test: 0.7688397347194403 -> 0.7851139328218831, p_value: 4.085106492228866e-259\ndel 1% training data, p@10 on test: 0.0001618996222342148 -> 0.00040474905558553697, p_value: 0.03893078117141074\ndel 2% training data, rmse on test: 0.913437474491702 -> 0.8876276245194641, p_value: 6.296705493023683e-93\ndel 2% training data, aucs on test: 0.7688397347194403 -> 0.7925921987584407, p_value: 0.0\ndel 2% training data, p@10 on test: 0.0001618996222342148 -> 0.00048569886670264445, p_value: 0.01428617256250652\ndel 5% training data, rmse on test: 0.913437474491702 -> 0.8856764963057757, p_value: 1.6931729682176503e-95\ndel 5% training data, aucs on test: 0.7688397347194403 -> 0.8025626775794855, p_value: 0.0\ndel 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0010523475445223963, p_value: 8.375339972336538e-07\ndel 10% training data, rmse on test: 0.913437474491702 -> 0.8889812277014134, p_value: 2.136473307357492e-63\ndel 10% training data, aucs on test: 0.7688397347194403 -> 0.8113526023186876, p_value: 0.0\ndel 10% training data, p@10 on test: 0.0001618996222342148 -> 0.0012142471667566108, p_value: 2.3774319121737882e-08\nmod 0.1% training data, rmse on test: 0.913437474491702 -> 0.9052936159794959, p_value: 4.6656114789352225e-26\nmod 0.1% training data, aucs on test: 0.7688397347194403 -> 0.7725780585060916, p_value: 6.7634807677642294e-77\nmod 0.1% training data, p@10 on test: 0.0001618996222342148 -> 0.0001618996222342148, p_value: 1.0\nmod 0.2% training data, rmse on test: 0.913437474491702 -> 0.9011219443120054, p_value: 4.068680574720488e-44\nmod 0.2% training data, aucs on test: 0.7688397347194403 -> 0.7760425280262492, p_value: 3.0295433227650017e-173\nmod 0.2% training data, p@10 on test: 0.0001618996222342148 -> 0.0002698327037236913, p_value: 0.3173758126932722\nmod 0.5% training data, rmse on test: 0.913437474491702 -> 0.8943853375909748, p_value: 3.104308044669012e-78\nmod 0.5% training data, aucs on test: 0.7688397347194403 -> 0.7818429313459041, p_value: 0.0\nmod 0.5% training data, p@10 on test: 0.0001618996222342148 -> 0.00029681597409606046, p_value: 0.22530076890732273\nmod 1% training data, rmse on test: 0.913437474491702 -> 0.8880276777116627, p_value: 3.166269018230575e-126\nmod 1% training data, aucs on test: 0.7688397347194403 -> 0.7887205859788688, p_value: 0.0\nmod 1% training data, p@10 on test: 0.0001618996222342148 -> 0.0004587155963302752, p_value: 0.016357281407813413\nmod 2% training data, rmse on test: 0.913437474491702 -> 0.8812064870149748, p_value: 8.249180264722856e-174\nmod 2% training data, aucs on test: 0.7688397347194403 -> 0.7977500988702074, p_value: 0.0\nmod 2% training data, p@10 on test: 0.0001618996222342148 -> 0.0004856988667026444, p_value: 0.010496730115705693\nmod 5% training data, rmse on test: 0.913437474491702 -> 0.8735115114954207, p_value: 9.268353110158512e-256\nmod 5% training data, aucs on test: 0.7688397347194403 -> 0.8056921888980414, p_value: 0.0\nmod 5% training data, p@10 on test: 0.0001618996222342148 -> 0.0027792768483540205, p_value: 3.9573738766261045e-19\nmod 10% training data, rmse on test: 0.913437474491702 -> 0.8695139528745185, p_value: 1.5706066008192407e-301\nmod 10% training data, aucs on test: 0.7688397347194403 -> 0.8102100454265159, p_value: 0.0\nmod 10% training data, p@10 on test: 0.0001618996222342148 -> 0.005774419859686995, p_value: 6.193606675786653e-36\n","output_type":"stream"}]},{"cell_type":"code","source":"modify_exp","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:26:43.746955Z","iopub.execute_input":"2023-03-26T20:26:43.747500Z","iopub.status.idle":"2023-03-26T20:26:43.766555Z","shell.execute_reply.started":"2023-03-26T20:26:43.747438Z","shell.execute_reply":"2023-03-26T20:26:43.765395Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\nmodify         0.1  0.905294  0.772578  0.000162\nmodify         0.2  0.901122  0.776043   0.00027\nmodify         0.5  0.894385  0.781843  0.000297\nmodify         1.0  0.888028  0.788721  0.000459\nmodify         2.0  0.881206   0.79775  0.000486\nmodify         5.0  0.873512  0.805692  0.002779\nmodify        10.0  0.869514   0.81021  0.005774","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>modify</th>\n      <td>0.1</td>\n      <td>0.905294</td>\n      <td>0.772578</td>\n      <td>0.000162</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.2</td>\n      <td>0.901122</td>\n      <td>0.776043</td>\n      <td>0.00027</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.5</td>\n      <td>0.894385</td>\n      <td>0.781843</td>\n      <td>0.000297</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>1.0</td>\n      <td>0.888028</td>\n      <td>0.788721</td>\n      <td>0.000459</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>2.0</td>\n      <td>0.881206</td>\n      <td>0.79775</td>\n      <td>0.000486</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>5.0</td>\n      <td>0.873512</td>\n      <td>0.805692</td>\n      <td>0.002779</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>10.0</td>\n      <td>0.869514</td>\n      <td>0.81021</td>\n      <td>0.005774</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"delete_exp","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:26:50.529891Z","iopub.execute_input":"2023-03-26T20:26:50.530819Z","iopub.status.idle":"2023-03-26T20:26:50.546717Z","shell.execute_reply.started":"2023-03-26T20:26:50.530764Z","shell.execute_reply":"2023-03-26T20:26:50.544572Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\ndelete         0.1  0.907387  0.772359  0.000216\ndelete         0.2  0.903348  0.774321  0.000216\ndelete         0.5  0.898293  0.780133  0.000351\ndelete         1.0  0.892717  0.785114  0.000405\ndelete         2.0  0.887628  0.792592  0.000486\ndelete         5.0  0.885676  0.802563  0.001052\ndelete        10.0  0.888981  0.811353  0.001214","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>delete</th>\n      <td>0.1</td>\n      <td>0.907387</td>\n      <td>0.772359</td>\n      <td>0.000216</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.2</td>\n      <td>0.903348</td>\n      <td>0.774321</td>\n      <td>0.000216</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.5</td>\n      <td>0.898293</td>\n      <td>0.780133</td>\n      <td>0.000351</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>1.0</td>\n      <td>0.892717</td>\n      <td>0.785114</td>\n      <td>0.000405</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>2.0</td>\n      <td>0.887628</td>\n      <td>0.792592</td>\n      <td>0.000486</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>5.0</td>\n      <td>0.885676</td>\n      <td>0.802563</td>\n      <td>0.001052</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>10.0</td>\n      <td>0.888981</td>\n      <td>0.811353</td>\n      <td>0.001214</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"args = tempo(mode = 'test', implicit = 'store_true', negative = False)\nmodify_pos, delete_pos = run(args)","metadata":{"execution":{"iopub.status.busy":"2023-03-26T20:26:52.763054Z","iopub.execute_input":"2023-03-26T20:26:52.763563Z","iopub.status.idle":"2023-03-26T22:32:36.852747Z","shell.execute_reply.started":"2023-03-26T20:26:52.763522Z","shell.execute_reply":"2023-03-26T22:32:36.851079Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  \n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  if sys.path[0] == \"\":\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nDeprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n  from ipykernel import kernelapp as app\n","output_type":"stream"},{"name":"stdout","text":"del 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18249644353091043, p_value: 0.0\ndel 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9266967009060519, p_value: 0.0001276966667295583\ndel 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.1351259552787999, p_value: 0.6625835311614025\ndel 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18241544387685954, p_value: 0.0\ndel 0.2% training data, aucs on test: 0.9267385280885827 -> 0.926672111344329, p_value: 0.0002249629032130969\ndel 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13453155958109259, p_value: 0.03523967584882992\ndel 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18217356975189314, p_value: 0.0\ndel 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9266177272628835, p_value: 4.315969578028615e-05\ndel 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13424851401075574, p_value: 0.019068586819048775\ndel 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18177593517801396, p_value: 0.0\ndel 1% training data, aucs on test: 0.9267385280885827 -> 0.9265707862287809, p_value: 0.00034402828823171646\ndel 1% training data, p@10 on test: 0.13523917350693462 -> 0.13405038211151996, p_value: 0.03073382833867656\ndel 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.181000143937043, p_value: 0.0\ndel 2% training data, aucs on test: 0.9267385280885827 -> 0.9264603107718078, p_value: 0.00020678016196438656\ndel 2% training data, p@10 on test: 0.13523917350693462 -> 0.13393716388338522, p_value: 0.04733036169293658\ndel 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.17875145221605454, p_value: 0.0\ndel 5% training data, aucs on test: 0.9267385280885827 -> 0.9256196911180123, p_value: 2.3420484230056785e-08\ndel 5% training data, p@10 on test: 0.13523917350693462 -> 0.1338522502122842, p_value: 0.177311549733292\ndel 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17528054669258408, p_value: 0.0\ndel 10% training data, aucs on test: 0.9267385280885827 -> 0.9238172008563675, p_value: 3.349805100382119e-19\ndel 10% training data, p@10 on test: 0.13523917350693462 -> 0.1309368808378149, p_value: 0.0010105145642007865\nmod 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.182544781391595, p_value: 0.0\nmod 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9267262201584202, p_value: 0.0019259884118490913\nmod 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.13518256439286727, p_value: 0.7518788905799987\nmod 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18251332371423598, p_value: 0.0\nmod 0.2% training data, aucs on test: 0.9267385280885827 -> 0.9267210282850065, p_value: 0.008282475125275702\nmod 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13475799603736202, p_value: 0.02686256152369693\nmod 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.1824209898498937, p_value: 0.0\nmod 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9267108151447911, p_value: 0.010338179664326779\nmod 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13504104160769886, p_value: 0.4679987006986216\nmod 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18227210574734334, p_value: 0.0\nmod 1% training data, aucs on test: 0.9267385280885827 -> 0.9267045349987055, p_value: 0.044903878957030866\nmod 1% training data, p@10 on test: 0.13523917350693462 -> 0.13436173223889047, p_value: 0.008535600607931688\nmod 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18198646125061801, p_value: 0.0\nmod 2% training data, aucs on test: 0.9267385280885827 -> 0.9267115058386151, p_value: 0.2626517436105023\nmod 2% training data, p@10 on test: 0.13523917350693462 -> 0.1344749504670252, p_value: 0.054383938401998844\nmod 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18116896142239164, p_value: 0.0\nmod 5% training data, aucs on test: 0.9267385280885827 -> 0.9267349802381507, p_value: 0.9304145391326415\nmod 5% training data, p@10 on test: 0.13523917350693462 -> 0.134135295782621, p_value: 0.03467041707178722\nmod 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17987895625367242, p_value: 0.0\nmod 10% training data, aucs on test: 0.9267385280885827 -> 0.9267673876531942, p_value: 0.6169991931918938\nmod 10% training data, p@10 on test: 0.13523917350693462 -> 0.1343334276818568, p_value: 0.1621662144650468\ndel 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18249644353091043, p_value: 0.0\ndel 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9266967009060519, p_value: 0.0001276966667295583\ndel 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.1351259552787999, p_value: 0.6625835311614025\ndel 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18241544387685954, p_value: 0.0\ndel 0.2% training data, aucs on test: 0.9267385280885827 -> 0.926672111344329, p_value: 0.0002249629032130969\ndel 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13453155958109259, p_value: 0.03523967584882992\ndel 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18217356975189314, p_value: 0.0\ndel 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9266177272628835, p_value: 4.315969578028615e-05\ndel 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13424851401075574, p_value: 0.019068586819048775\ndel 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18177593517801396, p_value: 0.0\ndel 1% training data, aucs on test: 0.9267385280885827 -> 0.9265707862287809, p_value: 0.00034402828823171646\ndel 1% training data, p@10 on test: 0.13523917350693462 -> 0.13405038211151996, p_value: 0.03073382833867656\ndel 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.181000143937043, p_value: 0.0\ndel 2% training data, aucs on test: 0.9267385280885827 -> 0.9264603107718078, p_value: 0.00020678016196438656\ndel 2% training data, p@10 on test: 0.13523917350693462 -> 0.13393716388338522, p_value: 0.04733036169293658\ndel 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.17875145221605454, p_value: 0.0\ndel 5% training data, aucs on test: 0.9267385280885827 -> 0.9256196911180123, p_value: 2.3420484230056785e-08\ndel 5% training data, p@10 on test: 0.13523917350693462 -> 0.1338522502122842, p_value: 0.177311549733292\ndel 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17528054669258408, p_value: 0.0\ndel 10% training data, aucs on test: 0.9267385280885827 -> 0.9238172008563675, p_value: 3.349805100382119e-19\ndel 10% training data, p@10 on test: 0.13523917350693462 -> 0.1309368808378149, p_value: 0.0010105145642007865\nmod 0.1% training data, weighted rmse on test: 0.18257745873775652 -> 0.182544781391595, p_value: 0.0\nmod 0.1% training data, aucs on test: 0.9267385280885827 -> 0.9267262201584202, p_value: 0.0019259884118490913\nmod 0.1% training data, p@10 on test: 0.13523917350693462 -> 0.13518256439286727, p_value: 0.7518788905799987\nmod 0.2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18251332371423598, p_value: 0.0\nmod 0.2% training data, aucs on test: 0.9267385280885827 -> 0.9267210282850065, p_value: 0.008282475125275702\nmod 0.2% training data, p@10 on test: 0.13523917350693462 -> 0.13475799603736202, p_value: 0.02686256152369693\nmod 0.5% training data, weighted rmse on test: 0.18257745873775652 -> 0.1824209898498937, p_value: 0.0\nmod 0.5% training data, aucs on test: 0.9267385280885827 -> 0.9267108151447911, p_value: 0.010338179664326779\nmod 0.5% training data, p@10 on test: 0.13523917350693462 -> 0.13504104160769886, p_value: 0.4679987006986216\nmod 1% training data, weighted rmse on test: 0.18257745873775652 -> 0.18227210574734334, p_value: 0.0\nmod 1% training data, aucs on test: 0.9267385280885827 -> 0.9267045349987055, p_value: 0.044903878957030866\nmod 1% training data, p@10 on test: 0.13523917350693462 -> 0.13436173223889047, p_value: 0.008535600607931688\nmod 2% training data, weighted rmse on test: 0.18257745873775652 -> 0.18198646125061801, p_value: 0.0\nmod 2% training data, aucs on test: 0.9267385280885827 -> 0.9267115058386151, p_value: 0.2626517436105023\nmod 2% training data, p@10 on test: 0.13523917350693462 -> 0.1344749504670252, p_value: 0.054383938401998844\nmod 5% training data, weighted rmse on test: 0.18257745873775652 -> 0.18116896142239164, p_value: 0.0\nmod 5% training data, aucs on test: 0.9267385280885827 -> 0.9267349802381507, p_value: 0.9304145391326415\nmod 5% training data, p@10 on test: 0.13523917350693462 -> 0.134135295782621, p_value: 0.03467041707178722\nmod 10% training data, weighted rmse on test: 0.18257745873775652 -> 0.17987895625367242, p_value: 0.0\nmod 10% training data, aucs on test: 0.9267385280885827 -> 0.9267673876531942, p_value: 0.6169991931918938\nmod 10% training data, p@10 on test: 0.13523917350693462 -> 0.1343334276818568, p_value: 0.1621662144650468\n","output_type":"stream"}]},{"cell_type":"code","source":"modify_pos","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:56:19.683591Z","iopub.execute_input":"2023-03-26T22:56:19.684570Z","iopub.status.idle":"2023-03-26T22:56:19.705438Z","shell.execute_reply.started":"2023-03-26T22:56:19.684519Z","shell.execute_reply":"2023-03-26T22:56:19.703831Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\nmodify         0.1  0.182545  0.926726  0.135183\nmodify         0.2  0.182513  0.926721  0.134758\nmodify         0.5  0.182421  0.926711  0.135041\nmodify         1.0  0.182272  0.926705  0.134362\nmodify         2.0  0.181986  0.926712  0.134475\nmodify         5.0  0.181169  0.926735  0.134135\nmodify        10.0  0.179879  0.926767  0.134333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>modify</th>\n      <td>0.1</td>\n      <td>0.182545</td>\n      <td>0.926726</td>\n      <td>0.135183</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.2</td>\n      <td>0.182513</td>\n      <td>0.926721</td>\n      <td>0.134758</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.5</td>\n      <td>0.182421</td>\n      <td>0.926711</td>\n      <td>0.135041</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>1.0</td>\n      <td>0.182272</td>\n      <td>0.926705</td>\n      <td>0.134362</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>2.0</td>\n      <td>0.181986</td>\n      <td>0.926712</td>\n      <td>0.134475</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>5.0</td>\n      <td>0.181169</td>\n      <td>0.926735</td>\n      <td>0.134135</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>10.0</td>\n      <td>0.179879</td>\n      <td>0.926767</td>\n      <td>0.134333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"delete_pos","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:56:23.984906Z","iopub.execute_input":"2023-03-26T22:56:23.985302Z","iopub.status.idle":"2023-03-26T22:56:24.002480Z","shell.execute_reply.started":"2023-03-26T22:56:23.985267Z","shell.execute_reply":"2023-03-26T22:56:24.000368Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\ndelete         0.1  0.182496  0.926697  0.135126\ndelete         0.2  0.182415  0.926672  0.134532\ndelete         0.5  0.182174  0.926618  0.134249\ndelete         1.0  0.181776  0.926571   0.13405\ndelete         2.0     0.181   0.92646  0.133937\ndelete         5.0  0.178751   0.92562  0.133852\ndelete        10.0  0.175281  0.923817  0.130937","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>delete</th>\n      <td>0.1</td>\n      <td>0.182496</td>\n      <td>0.926697</td>\n      <td>0.135126</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.2</td>\n      <td>0.182415</td>\n      <td>0.926672</td>\n      <td>0.134532</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.5</td>\n      <td>0.182174</td>\n      <td>0.926618</td>\n      <td>0.134249</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>1.0</td>\n      <td>0.181776</td>\n      <td>0.926571</td>\n      <td>0.13405</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>2.0</td>\n      <td>0.181</td>\n      <td>0.92646</td>\n      <td>0.133937</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>5.0</td>\n      <td>0.178751</td>\n      <td>0.92562</td>\n      <td>0.133852</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>10.0</td>\n      <td>0.175281</td>\n      <td>0.923817</td>\n      <td>0.130937</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"args = tempo(mode = 'test', implicit = 'store_true', negative = True)\nmodify_neg, delete_neg = run(args)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modify_neg","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:32:36.855188Z","iopub.execute_input":"2023-03-26T22:32:36.855713Z","iopub.status.idle":"2023-03-26T22:32:36.873886Z","shell.execute_reply.started":"2023-03-26T22:32:36.855668Z","shell.execute_reply":"2023-03-26T22:32:36.871955Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\nmodify         0.1  0.306171  0.918845  0.133909\nmodify         0.2  0.306104  0.918866  0.133739\nmodify         0.5  0.306151  0.919156  0.134079\nmodify         1.0  0.305972  0.918996  0.135324\nmodify         2.0   0.30585  0.918912  0.136541\nmodify         5.0  0.305231  0.918812  0.133881\nmodify        10.0  0.304239  0.918624  0.134984","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>modify</th>\n      <td>0.1</td>\n      <td>0.306171</td>\n      <td>0.918845</td>\n      <td>0.133909</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.2</td>\n      <td>0.306104</td>\n      <td>0.918866</td>\n      <td>0.133739</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>0.5</td>\n      <td>0.306151</td>\n      <td>0.919156</td>\n      <td>0.134079</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>1.0</td>\n      <td>0.305972</td>\n      <td>0.918996</td>\n      <td>0.135324</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>2.0</td>\n      <td>0.30585</td>\n      <td>0.918912</td>\n      <td>0.136541</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>5.0</td>\n      <td>0.305231</td>\n      <td>0.918812</td>\n      <td>0.133881</td>\n    </tr>\n    <tr>\n      <th>modify</th>\n      <td>10.0</td>\n      <td>0.304239</td>\n      <td>0.918624</td>\n      <td>0.134984</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"delete_neg","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:32:36.875942Z","iopub.execute_input":"2023-03-26T22:32:36.876412Z","iopub.status.idle":"2023-03-26T22:32:36.895174Z","shell.execute_reply.started":"2023-03-26T22:32:36.876368Z","shell.execute_reply":"2023-03-26T22:32:36.893786Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"        percentage      rmse   roc_auc precision\ndelete         0.1  0.306161  0.918933   0.13405\ndelete         0.2  0.306133  0.918737  0.134305\ndelete         0.5  0.305913   0.91866  0.135664\ndelete         1.0  0.305538  0.918687  0.132975\ndelete         2.0  0.304937  0.918381  0.132862\ndelete         5.0  0.303213  0.917367  0.134022\ndelete        10.0  0.300143  0.916284   0.13272","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>percentage</th>\n      <th>rmse</th>\n      <th>roc_auc</th>\n      <th>precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>delete</th>\n      <td>0.1</td>\n      <td>0.306161</td>\n      <td>0.918933</td>\n      <td>0.13405</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.2</td>\n      <td>0.306133</td>\n      <td>0.918737</td>\n      <td>0.134305</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>0.5</td>\n      <td>0.305913</td>\n      <td>0.91866</td>\n      <td>0.135664</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>1.0</td>\n      <td>0.305538</td>\n      <td>0.918687</td>\n      <td>0.132975</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>2.0</td>\n      <td>0.304937</td>\n      <td>0.918381</td>\n      <td>0.132862</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>5.0</td>\n      <td>0.303213</td>\n      <td>0.917367</td>\n      <td>0.134022</td>\n    </tr>\n    <tr>\n      <th>delete</th>\n      <td>10.0</td>\n      <td>0.300143</td>\n      <td>0.916284</td>\n      <td>0.13272</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"modify_pos.to_csv('modify_pos.csv')\ndelete_pos.to_csv('delete_pos.csv')\nfrom IPython.display import FileLink\nFileLink('delete_pos.csv')\n","metadata":{"execution":{"iopub.status.busy":"2023-03-26T22:57:02.009665Z","iopub.execute_input":"2023-03-26T22:57:02.010117Z","iopub.status.idle":"2023-03-26T22:57:02.023885Z","shell.execute_reply.started":"2023-03-26T22:57:02.010077Z","shell.execute_reply":"2023-03-26T22:57:02.022185Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/delete_pos.csv","text/html":"<a href='delete_pos.csv' target='_blank'>delete_pos.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}